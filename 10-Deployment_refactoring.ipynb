{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import string\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create spacy word2vec and list of stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# load classifier\n",
    "with open('temp/RFClassifier.pkl', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "# load vectorizer\n",
    "with open('temp/TFIDFVectorizer_lemma.pkl', 'rb') as f:\n",
    "    v_lemma = pickle.load(f)\n",
    "with open('temp/TFIDFVectorizer_keyword.pkl', 'rb') as f:\n",
    "    v_keyword = pickle.load(f)\n",
    "with open('temp/TFIDFVectorizer_noun.pkl', 'rb') as f:\n",
    "    v_noun = pickle.load(f)\n",
    "with open('temp/TFIDFVectorizer_verb.pkl', 'rb') as f:\n",
    "    v_verb = pickle.load(f)\n",
    "\n",
    "# load intent list\n",
    "with open('temp/intent_list.pkl', 'rb') as f:\n",
    "    intent_list = pickle.load(f)\n",
    "\n",
    "# load clustering centres\n",
    "with open('temp/dict_cluster.pkl', 'rb') as f:\n",
    "    dict_cluster = pickle.load(f)\n",
    "\n",
    "# load word2vec\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load('word2vec-google-news-300')\n",
    "\n",
    "# load idf\n",
    "with open('temp/idf.pkl', 'rb') as f:\n",
    "    idf = pickle.load(f)\n",
    "\n",
    "# load intent2index\n",
    "with open('temp/intent2index.pkl', 'rb') as f:\n",
    "    intent2index = pickle.load(f)\n",
    "    \n",
    "# load keyword_list_lemma\n",
    "with open('temp/keyword_list_lemma.pkl', 'rb') as f:\n",
    "    keyword_list_lemma = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "def get_nlp_features(df, keyword_list_lemma):\n",
    "    \"\"\" Get keyword features from dataframe \"\"\"\n",
    "    data = df.copy()\n",
    "    data['lemma'] = data['query'].apply(lambda x:' '.join([token.lemma_ for token in nlp(x) if token.lemma_ not in stop_words]))\n",
    "    data['keyword'] = data['lemma'].apply(lambda x: list(set([token.lemma_ for token in nlp(x) if token.lemma_ in keyword_list_lemma])))\n",
    "\n",
    "    data['noun'] = data['query'].apply(lambda x: list(set([token.lemma_ for token in nlp(x) if token.pos_ in ['NOUN','PROPN'] and token.lemma_ not in stop_words])))\n",
    "    data['verb'] = data['query'].apply(lambda x: list(set([token.lemma_ for token in nlp(x) if token.pos_ in ['VERB'] and token.lemma_ not in stop_words])))\n",
    "\n",
    "    data['noun'] = data['noun'].apply(lambda x: ' '.join([w for w in x]))\n",
    "    data['verb'] = data['verb'].apply(lambda x: ' '.join([w for w in x]))\n",
    "    data['keyword'] = data['keyword'].apply(lambda x: ' '.join([w for w in x]))\n",
    "    return data\n",
    "\n",
    "def get_distance_matrix_idf(df_test, intent_list, dict_cluster, word2vec, idf):\n",
    "    \"\"\" Get distance for each query to every intent center\n",
    "        \n",
    "    Args:\n",
    "        df_test (pd.DataFrame): input test dataframe with intent and query\n",
    "        intent_list (list): list of intents to loop through\n",
    "        dict_cluster (dict): dictionary of cluster centres\n",
    "        word2vec (dict): word embeddings dictionary\n",
    "        idf (dict): idf of each words\n",
    "\n",
    "    Returns:\n",
    "        result (pd.DataFrame): distance matrix for each query, lowest distance intent idealy should match label\n",
    "    \"\"\"\n",
    "    df = df_test.copy()\n",
    "    for intent in intent_list:\n",
    "        # distance = cosine_similarity(sentence embedding, intent cluster centre embedding)\n",
    "        df[intent] = df['query'].apply(lambda x: cosine_distances(get_sentence_vec(x, word2vec, idf).reshape(1,-1), \n",
    "                                                                  dict_cluster[intent].reshape(1,-1)).item())\n",
    "    return df\n",
    "\n",
    "def get_sentence_vec(sentence, word2vec, idf=None):\n",
    "    \"\"\" Get embedding of sentence by using word2vec embedding of words\n",
    "    \n",
    "    If idf is provided, the sentence is the weighted embedding by\n",
    "        SUM( embedding[word] x idf[word] )\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): input sentence\n",
    "        word2vec (dict): loaded word2vec model from Gensim\n",
    "        idf (dict, optional): inverse document frequency of words in all queries\n",
    "\n",
    "    Returns:\n",
    "        emb (np.array): 300-dimentions embedding of sentence\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    words = [word for word in words if word in word2vec.vocab]\n",
    "    \n",
    "    # if no word in word2vec vocab, return 0x300 embedding\n",
    "    if len(words)==0:\n",
    "        return np.zeros((300,), dtype='float32')\n",
    "    \n",
    "    # use mean if no idf provided\n",
    "    if idf is None:\n",
    "        emb = word2vec[words].mean(axis=0)\n",
    "    else:\n",
    "        # get all idf of words, if new word is not in idf, assign 0.0 weights\n",
    "        idf_series = np.array([idf.get(word, 0.0) for word in words])\n",
    "        # change shape to 1 x num_of_words\n",
    "        idf_series = idf_series.reshape(1, -1)\n",
    "        # use matrix multiplication to get weighted word vector sum for sentence embeddings\n",
    "        emb = np.matmul(idf_series, word2vec[words]).reshape(-1)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" Basic text cleaning\n",
    "        \n",
    "        1. lowercase\n",
    "        2. remove special characters\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def nltk_tokenize(text):\n",
    "    \"\"\" tokenize text using NLTK and join back as sentence\"\"\"\n",
    "    # import nltk\n",
    "    # nltk.download('punkt')\n",
    "    return ' '.join(word_tokenize(text))\n",
    "\n",
    "\n",
    "def get_top_3(data, intent_list):\n",
    "    data = data.copy()\n",
    "    cluster_cols = intent_list.copy()\n",
    "\n",
    "    data['clusters_top3'] = data.apply(lambda x: np.argsort(x[cluster_cols].values)[:3].tolist(), axis=1)\n",
    "\n",
    "    top_clusters_cols = pd.DataFrame(data['clusters_top3'].values.tolist(),columns = ['clusters_1','clusters_2','clusters_3']).reset_index(drop=True)\n",
    "    data = data.reset_index(drop=True)\n",
    "    data = pd.concat([data,top_clusters_cols], axis=1)\n",
    "\n",
    "    data.drop(columns = 'clusters_top3', inplace=True)\n",
    "    data.drop(columns = cluster_cols, inplace=True)\n",
    "    \n",
    "    # print(data.head())\n",
    "    return data\n",
    "\n",
    "def add_nlp_vec(df, v_lemma, v_keyword, v_noun, v_verb, top_clusters_cols):\n",
    "    \"\"\" Transform NLP features to vector for input X using TFIDF \"\"\"\n",
    "    x_test_lemma = v_lemma.transform(df['lemma'])\n",
    "    x_test_keyword = v_keyword.transform(df['keyword'])\n",
    "    x_test_noun = v_noun.transform(df['noun'])\n",
    "    x_test_verb = v_verb.transform(df['verb'])\n",
    "    \n",
    "    # combine all features \n",
    "    x_test_combined = hstack((x_test_lemma,\n",
    "                              x_test_keyword,\n",
    "                              x_test_noun,\n",
    "                              x_test_verb,\n",
    "                              df[top_clusters_cols].values),format='csr')\n",
    "\n",
    "    x_test_combined_columns = v_lemma.get_feature_names()+\\\n",
    "                              v_keyword.get_feature_names()+\\\n",
    "                              v_noun.get_feature_names()+\\\n",
    "                              v_verb.get_feature_names()+\\\n",
    "                              top_clusters_cols\n",
    "    \n",
    "    x_test_combined = pd.DataFrame(x_test_combined.toarray())\n",
    "    x_test_combined.columns = x_test_combined_columns\n",
    "    \n",
    "    return x_test_combined\n",
    "\n",
    "def get_target_name(index, index2intent):\n",
    "    return index2intent[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent_nlp_clustering(query):\n",
    "    \"\"\" load classification model outside the function  \n",
    "        \n",
    "        return a dataframe df\n",
    "        columns: pred_seq, intent_class, intent_string, pred_prob\n",
    "        rows: top 3 prediciton, example for first row: 1, 0, Promotions, 0.66\n",
    "    \"\"\"\n",
    "    # setup timer\n",
    "    start = time.time()\n",
    "\n",
    "    #%% pipeline\n",
    "    # convert question to dataframe\n",
    "    df = pd.DataFrame()\n",
    "    df = pd.DataFrame(columns=['query'])\n",
    "    df.loc[0] = [query]\n",
    "\n",
    "    # preprocessing test query\n",
    "    df['query'] = df['query'].apply(clean_text)\n",
    "    df['query'] = df['query'].apply(nltk_tokenize)\n",
    "    df['query'] = df['query'].apply(lambda x:' '.join([token.lemma_ for token in nlp(x) if token.lemma_ not in stop_words]))\n",
    "    df['query'] = df['query'].str.lower()\n",
    "    \n",
    "    # get nlp features\n",
    "    df = get_nlp_features(df, keyword_list_lemma)\n",
    "\n",
    "    # get clustering matrix\n",
    "    df_cluster = get_distance_matrix_idf(df, intent_list, dict_cluster, word2vec, idf)\n",
    "\n",
    "    # get top 3 clusters\n",
    "    top_3 = get_top_3(df_cluster, intent_list)\n",
    "    print(top_3)\n",
    "\n",
    "    # get inputs for RF classifier\n",
    "    countvector_cols = ['lemma', 'keyword', 'noun', 'verb']\n",
    "    top_clusters_cols = ['clusters_1', 'clusters_2', 'clusters_3']\n",
    "    feature_cols = countvector_cols + top_clusters_cols\n",
    "    \n",
    "    X_in = add_nlp_vec(top_3, v_lemma, v_keyword, v_noun, v_verb, top_clusters_cols)\n",
    "\n",
    "    # get prediction proba\n",
    "    probs = clf.predict_proba(X_in)\n",
    "\n",
    "    # get index for top 3 prediction by proba\n",
    "    ind = np.argsort(probs, axis=1)[:,-3:]\n",
    "\n",
    "    # save probability\n",
    "    proba = probs[0][ind[0]]\n",
    "\n",
    "    # save predicitons as dataframe\n",
    "    best_3 = pd.DataFrame(ind,columns=['top3','top2','top1'])\n",
    "    best_3['top1'] = clf.classes_[best_3['top1']]\n",
    "    best_3['top2'] = clf.classes_[best_3['top2']]\n",
    "    best_3['top3'] = clf.classes_[best_3['top3']]\n",
    "    best_3['top3_prob'] = proba[0]\n",
    "    best_3['top2_prob'] = proba[1]\n",
    "    best_3['top1_prob'] = proba[2]\n",
    "\n",
    "    # get index to intent dictionary from intent2index\n",
    "    index2intent = {y:x for x,y in intent2index.items()}\n",
    "\n",
    "    # get class name of top predictions\n",
    "    best_3['top1_name'] = best_3['top1'].apply(get_target_name, index2intent=index2intent)\n",
    "    best_3['top2_name'] = best_3['top2'].apply(get_target_name, index2intent=index2intent)\n",
    "    best_3['top3_name'] = best_3['top3'].apply(get_target_name, index2intent=index2intent)\n",
    "\n",
    "    # output prediction\n",
    "    top1 = best_3.at[0,'top1_name']\n",
    "    top2 = best_3.at[0,'top2_name']\n",
    "    top3 = best_3.at[0,'top3_name']\n",
    "    top1_prob = best_3.at[0,'top1_prob']\n",
    "    top2_prob = best_3.at[0,'top2_prob']\n",
    "    top3_prob = best_3.at[0,'top3_prob']\n",
    "\n",
    "    print(f'For sentence:\\n{query}\\n')\n",
    "    print(f'Top 1 prediction intent is {top1} with probability {100*top1_prob:.2f}%')\n",
    "    print(f'Top 2 prediction intent is {top2} with probability {100*top2_prob:.2f}%')\n",
    "    print(f'Top 3 prediction intent is {top3} with probability {100*top3_prob:.2f}%')\n",
    "\n",
    "    top1_class = best_3.at[0,'top1']\n",
    "    top2_class = best_3.at[0,'top2']\n",
    "    top3_class = best_3.at[0,'top3']\n",
    "\n",
    "    # convert to output\n",
    "    df = pd.DataFrame([\n",
    "            [1, top1_class, top1, top1_prob],\n",
    "            [2, top2_class, top2, top2_prob],\n",
    "            [3, top3_class, top3, top3_prob]\n",
    "        ], columns=['pred_seq', 'intent_class', 'intent', 'pred_prob'])\n",
    "\n",
    "    inference_time = time.time() - start\n",
    "    return df, inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"Please show me the current promotions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      query                     lemma    keyword  \\\n",
      "0  -pron- current promotion  -pron- current promotion  promotion   \n",
      "\n",
      "               noun verb  clusters_1  clusters_2  clusters_3  \n",
      "0  -pron- promotion               45          46          56  \n",
      "For sentence:\n",
      "Please show me the current promotions\n",
      "\n",
      "Top 1 prediction intent is Promotions with probability 98.50%\n",
      "Top 2 prediction intent is Card Promotions with probability 1.10%\n",
      "Top 3 prediction intent is How to redeem rewards with probability 0.10%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   pred_seq  intent_class                 intent  pred_prob\n",
       " 0         1            45             Promotions      0.985\n",
       " 1         2            46        Card Promotions      0.011\n",
       " 2         3            75  How to redeem rewards      0.001,\n",
       " 0.34291529655456543)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_intent_nlp_clustering(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alert]",
   "language": "python",
   "name": "conda-env-alert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
