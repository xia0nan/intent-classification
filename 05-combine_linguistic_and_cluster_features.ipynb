{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T02:34:21.261399Z",
     "start_time": "2020-04-02T02:34:09.727122Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "import re\n",
    "import plotly.express as px\n",
    "\n",
    "import os,json\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xlrd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler \n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T02:50:10.452868Z",
     "start_time": "2020-04-02T02:50:10.400110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Statement request</th>\n",
       "      <th>Passbook savings accounts</th>\n",
       "      <th>Card statements</th>\n",
       "      <th>Credit card statement</th>\n",
       "      <th>Debit card statement</th>\n",
       "      <th>Investment account statement</th>\n",
       "      <th>Home loan account statement</th>\n",
       "      <th>...</th>\n",
       "      <th>Paying a cancelled credit card</th>\n",
       "      <th>How to close my account</th>\n",
       "      <th>Card dispute</th>\n",
       "      <th>Change credit card limit</th>\n",
       "      <th>Increase credit card limit</th>\n",
       "      <th>Decrease credit card limit</th>\n",
       "      <th>Credit card application rejection</th>\n",
       "      <th>Rebates</th>\n",
       "      <th>How to redeem rewards</th>\n",
       "      <th>Update details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Statement request</td>\n",
       "      <td>like copy -pron- statement</td>\n",
       "      <td>0.239015</td>\n",
       "      <td>0.476868</td>\n",
       "      <td>0.337956</td>\n",
       "      <td>0.419239</td>\n",
       "      <td>0.569444</td>\n",
       "      <td>0.497531</td>\n",
       "      <td>0.539970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.717396</td>\n",
       "      <td>0.724538</td>\n",
       "      <td>0.644826</td>\n",
       "      <td>0.740849</td>\n",
       "      <td>0.791113</td>\n",
       "      <td>0.835014</td>\n",
       "      <td>0.631769</td>\n",
       "      <td>0.770484</td>\n",
       "      <td>0.854625</td>\n",
       "      <td>0.560702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Statement request</td>\n",
       "      <td>send -pron- copy -pron- statement</td>\n",
       "      <td>0.267362</td>\n",
       "      <td>0.469997</td>\n",
       "      <td>0.365053</td>\n",
       "      <td>0.366122</td>\n",
       "      <td>0.528491</td>\n",
       "      <td>0.406568</td>\n",
       "      <td>0.507421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626802</td>\n",
       "      <td>0.666503</td>\n",
       "      <td>0.622732</td>\n",
       "      <td>0.720149</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.784794</td>\n",
       "      <td>0.599908</td>\n",
       "      <td>0.718440</td>\n",
       "      <td>0.817008</td>\n",
       "      <td>0.487535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             Intent                          Questions  \\\n",
       "0           0  Statement request         like copy -pron- statement   \n",
       "1           1  Statement request  send -pron- copy -pron- statement   \n",
       "\n",
       "   Statement request  Passbook savings accounts  Card statements  \\\n",
       "0           0.239015                   0.476868         0.337956   \n",
       "1           0.267362                   0.469997         0.365053   \n",
       "\n",
       "   Credit card statement  Debit card statement  Investment account statement  \\\n",
       "0               0.419239              0.569444                      0.497531   \n",
       "1               0.366122              0.528491                      0.406568   \n",
       "\n",
       "   Home loan account statement  ...  Paying a cancelled credit card  \\\n",
       "0                     0.539970  ...                        0.717396   \n",
       "1                     0.507421  ...                        0.626802   \n",
       "\n",
       "   How to close my account  Card dispute  Change credit card limit  \\\n",
       "0                 0.724538      0.644826                  0.740849   \n",
       "1                 0.666503      0.622732                  0.720149   \n",
       "\n",
       "   Increase credit card limit  Decrease credit card limit  \\\n",
       "0                    0.791113                    0.835014   \n",
       "1                    0.724000                    0.784794   \n",
       "\n",
       "   Credit card application rejection   Rebates  How to redeem rewards  \\\n",
       "0                           0.631769  0.770484               0.854625   \n",
       "1                           0.599908  0.718440               0.817008   \n",
       "\n",
       "   Update details  \n",
       "0        0.560702  \n",
       "1        0.487535  \n",
       "\n",
       "[2 rows x 80 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data_leave_one_out.csv')\n",
    "data = data.rename(columns = {'intent':'Intent','query':'Questions'})\n",
    "data = data.dropna().drop_duplicates()\n",
    "data['Questions'] = data['Questions'].apply(lambda x: x.lower())\n",
    "data['Questions']  = data['Questions'].apply(lambda x: re.sub(r'(!|\\.|,|\\(|\\)|\\[|\\]|\\\\|\\?|\\$|#|%|\\*)', '', x))\n",
    "data['Intent'] = data['Intent'].astype(str)\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T02:50:11.041246Z",
     "start_time": "2020-04-02T02:50:11.030099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_cols =['Promotions', 'Card Promotions',\n",
    "       'Open Account', 'OCBC Singapore Account', 'OCBC Securities Account ',\n",
    "       'OCBC Malaysia Account', 'NISP Account', 'Card Cancellation',\n",
    "       'Cancel Credit or Debit Card', 'Cancel ATM Card',\n",
    "       'Speak with a customer service officer', 'Request for sponsorship',\n",
    "       'Repricing', 'Auto Loan', 'Home Loan', 'Service Fee',\n",
    "       'Token Replacement', 'Token Activation', 'Hardware Token', 'Onetoken',\n",
    "       'Late fee waiver', 'Credit card interest waiver',\n",
    "       'Request for account fee waiver', 'Uplift suspension on accounts',\n",
    "       'Loan Enquiry', 'Home and property loans', 'Personal Loans',\n",
    "       '365 Card Application', 'Paying a cancelled credit card',\n",
    "       'How to close my account', 'Credit card transaction dispute',\n",
    "       'Change credit card limit', 'Increase credit card limit',\n",
    "       'Decrease credit card limit', 'Credit card application rejection',\n",
    "       'Rebates', 'How to redeem rewards', '360 Account interest dispute',\n",
    "       'Statement Request', 'Passbook savings account statement',\n",
    "       'Credit card statement', 'Debit card statement',\n",
    "       'Investment account statement', 'Update details']\n",
    "len(cluster_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_cols =[\n",
    "    'Statement request',\n",
    "    'Passbook savings accounts',\n",
    "    'Card statements',\n",
    "    'Credit card statement',\n",
    "    'Debit card statement',\n",
    "    'Investment account statement',\n",
    "    'Home loan account statement',\n",
    "    '360 Account interest dispute',\n",
    "    'Change of billing cycle',\n",
    "    'Token Activation',\n",
    "    'Student Loan',\n",
    "    'Tuition fee loan',\n",
    "    'Education loan',\n",
    "    'Study loan',\n",
    "    'Car loan full settlement',\n",
    "    'Home loan repayment',\n",
    "    'Cancel Fund Transfer',\n",
    "    'Cancel credit card transaction',\n",
    "    'Credit Refund',\n",
    "    'Account opening for foreigners',\n",
    "    'Mobile Banking Issues',\n",
    "    'Account Fraud',\n",
    "    'Dormant Account Activation',\n",
    "    'CRS Enquiries',\n",
    "    'SRS Contribution',\n",
    "    'Dispute status',\n",
    "    'Give a compliment',\n",
    "    'File a complaint',\n",
    "    'Funds Transfer Status',\n",
    "    'Telegraphic transfer Status',\n",
    "    'Make a telegraphic transfer',\n",
    "    'Unable to log into internet banking',\n",
    "    'Card application status',\n",
    "    'Supplementary card application',\n",
    "    'Access codes for banking services',\n",
    "    'Interest or Late fee waiver',\n",
    "    'Annual Fee Waiver',\n",
    "    'SMS Alerts',\n",
    "    'Reset PIN',\n",
    "    'Unsuccessful card transaction',\n",
    "    'Card Renewal',\n",
    "    'Card activation for overseas use',\n",
    "    'Replace Card',\n",
    "    'Lost or compromised cards',\n",
    "    'Damaged or Faulty card',\n",
    "    'Promotions',\n",
    "    'Card Promotions',\n",
    "    'Open Account',\n",
    "    'Open OCBC Singapore Account',\n",
    "    'Open OCBC Securities Account ',\n",
    "    'Open OCBC Malaysia Account',\n",
    "    'Open NISP Account',\n",
    "    'Card Cancellation',\n",
    "    'Cancel Credit or Debit Card',\n",
    "    'Cancel ATM Card',\n",
    "    'Speak with a customer service officer',\n",
    "    'Request for sponsorship',\n",
    "    'Repricing',\n",
    "    'Reprice home loan',\n",
    "    'Service Fee',\n",
    "    'Token Replacement',\n",
    "    'Request for account fee waiver',\n",
    "    'Uplift suspension on accounts',\n",
    "    'Loan Enquiry',\n",
    "    'Card Application',\n",
    "    'Apply for credit or debit cards',\n",
    "    'Apply for ATM card',\n",
    "    'Paying a cancelled credit card',\n",
    "    'How to close my account',\n",
    "    'Card dispute',\n",
    "    'Change credit card limit',\n",
    "    'Increase credit card limit',\n",
    "    'Decrease credit card limit',\n",
    "    'Credit card application rejection',\n",
    "    'Rebates',\n",
    "    'How to redeem rewards',\n",
    "    'Update details']\n",
    "len(cluster_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T02:50:12.349979Z",
     "start_time": "2020-04-02T02:50:11.855212Z"
    }
   },
   "outputs": [],
   "source": [
    "data['clusters_top3'] = data.apply(lambda x: np.argsort(x[cluster_cols].values)[:3].tolist(), axis=1)\n",
    "\n",
    "intents = cluster_cols # get all tickers\n",
    "intent2index = {v: i for (i, v) in enumerate(intents)}\n",
    "\n",
    "data['target'] = data['Intent'].apply(lambda x: intent2index[x])\n",
    "\n",
    "top_clusters_cols = pd.DataFrame(data['clusters_top3'].values.tolist(),columns = ['clusters_1','clusters_2','clusters_3']).reset_index(drop=True)\n",
    "data = data.reset_index(drop=True)\n",
    "data = pd.concat([data,top_clusters_cols], axis=1)\n",
    "\n",
    "data.drop(columns = 'clusters_top3', inplace=True)\n",
    "data.drop(columns = cluster_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Questions</th>\n",
       "      <th>target</th>\n",
       "      <th>clusters_1</th>\n",
       "      <th>clusters_2</th>\n",
       "      <th>clusters_3</th>\n",
       "      <th>exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Statement request</td>\n",
       "      <td>like copy -pron- statement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Statement request</td>\n",
       "      <td>send -pron- copy -pron- statement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Statement request</td>\n",
       "      <td>-pron- statement</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Statement request</td>\n",
       "      <td>want hard copy -pron- statement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Statement request</td>\n",
       "      <td>statement request</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             Intent                          Questions  target  \\\n",
       "0           0  Statement request         like copy -pron- statement       0   \n",
       "1           1  Statement request  send -pron- copy -pron- statement       0   \n",
       "2           2  Statement request                   -pron- statement       0   \n",
       "3           3  Statement request    want hard copy -pron- statement       0   \n",
       "4           4  Statement request                  statement request       0   \n",
       "\n",
       "   clusters_1  clusters_2  clusters_3  exists  \n",
       "0           0           2           3    True  \n",
       "1           0           2           3    True  \n",
       "2           2           0           3    True  \n",
       "3           0           2           3    True  \n",
       "4           0           2           3    True  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T02:50:12.484656Z",
     "start_time": "2020-04-02T02:50:12.476771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8003120124804992"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cluster method accuracy - top 1\n",
    "data[(data['clusters_1'] == data['target'])].shape[0] / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.890795631825273"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 2 accuracy\n",
    "data[\"exists\"] = data.drop(data.columns[[0,1,2,3,6]], 1).isin(data[\"target\"]).any(1)\n",
    "sum(data['exists'])/ data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9235569422776911"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 3 accuracy\n",
    "data[\"exists\"] = data.drop(data.columns[[0,1,2,3]], 1).isin(data[\"target\"]).any(1)\n",
    "sum(data['exists'])/ data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load nlp model and get stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T02:37:22.733762Z",
     "start_time": "2020-04-02T02:37:19.862943Z"
    }
   },
   "outputs": [],
   "source": [
    "# load spacy model \n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stop_words = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get keyword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T02:50:16.683112Z",
     "start_time": "2020-04-02T02:50:16.652338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interest', 'statement', 'apply', 'home', 'tuition', 'pay', 'issue', 'service', 'dispute', 'internet', 'unable', 'transfer', 'reprice', 'pin', 'home', 'enquiry', 'damage', 'card', 'service', 'rebate', 'singapore', 'rejection', 'officer', 'ocbc', 'compliment', 'update', 'use', 'atm', 'customer', 'open', 'study', 'credit', 'activation', 'sponsorship', 'application', 'malaysia', 'compromise', 'suspension', 'loan', 'passbook', 'request', 'telegraphic', 'enquiries', 'statement', 'investment', 'debit', 'dispute', 'statement', 'fee', 'cancellation', 'transfer', 'close', 'mobile', 'log', 'annual', 'billing', 'banking', 'repayment', 'waiver', 'cancel', 'reward', 'status', 'student', 'increase', 'contribution', 'application', 'speak', 'telegraphic', 'card', 'access', 'account', 'token', 'waiver', 'card', 'opening', 'file', 'code', 'foreigner', 'refund', 'education', 'supplementary', 'cancel', 'uplift', 'interest', 'replace', 'security', 'banking', 'decrease', 'lose', 'srs', 'savings', 'car', 'late', 'overseas', 'limit', 'loan', 'nisp', 'fund', 'activation', 'settlement', 'crs', 'detail', 'redeem', 'debit', 'request', 'fee', 'status', 'account', 'transaction', 'service', '360', 'alert', 'promotion', 'reprice', 'renewal', 'cycle', 'reset', 'faulty', 'change', 'credit', 'sms', 'account', 'fund', 'replacement', 'dormant', 'complaint', 'unsuccessful', 'fraud', 'nsip']\n"
     ]
    }
   ],
   "source": [
    "keywords = []\n",
    "for intent in list(set(data['Intent'])):\n",
    "    keywords.extend(intent.strip().split(' '))\n",
    "keyword_list = list(set(keywords))\n",
    "keyword_list = [i.lower() for i in keyword_list if i.lower() not in stop_words]\n",
    "keyword_list.append('nsip')\n",
    "\n",
    "keyword_list_lemma = []\n",
    "text = nlp(' '.join([w for w in keyword_list]))\n",
    "for token in text:\n",
    "    keyword_list_lemma.append(token.lemma_)\n",
    "print(keyword_list_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Linguistic feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T02:50:43.732972Z",
     "start_time": "2020-04-02T02:50:18.591185Z"
    }
   },
   "outputs": [],
   "source": [
    "data['lemma'] = data['Questions'].apply(lambda x:' '.join([token.lemma_ for token in nlp(x) if token.lemma_ not in stop_words]))\n",
    "data['keyword'] = data['lemma'].apply(lambda x: list(set([token.lemma_ for token in nlp(x) if token.lemma_ in keyword_list_lemma])))\n",
    "\n",
    "data['noun'] = data['Questions'].apply(lambda x: list(set([token.lemma_ for token in nlp(x) if token.pos_ in ['NOUN','PROPN'] and token.lemma_ not in stop_words])))\n",
    "data['verb'] = data['Questions'].apply(lambda x: list(set([token.lemma_ for token in nlp(x) if token.pos_ in ['VERB'] and token.lemma_ not in stop_words])))\n",
    "data['adj'] = data['Questions'].apply(lambda x: list(set([token.lemma_ for token in nlp(x) if token.pos_ in ['ADJ'] and token.lemma_ not in stop_words])))\n",
    "\n",
    "data['noun'] = data['noun'].apply(lambda x: ' '.join([w for w in x]))\n",
    "data['verb'] = data['verb'].apply(lambda x: ' '.join([w for w in x]))\n",
    "data['adj'] = data['adj'].apply(lambda x: ' '.join([w for w in x]))\n",
    "data['keyword'] = data['keyword'].apply(lambda x: ' '.join([w for w in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T02:50:43.751922Z",
     "start_time": "2020-04-02T02:50:43.736961Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Questions</th>\n",
       "      <th>target</th>\n",
       "      <th>clusters_1</th>\n",
       "      <th>clusters_2</th>\n",
       "      <th>clusters_3</th>\n",
       "      <th>lemma</th>\n",
       "      <th>keyword</th>\n",
       "      <th>noun</th>\n",
       "      <th>verb</th>\n",
       "      <th>adj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>368</td>\n",
       "      <td>Open Account</td>\n",
       "      <td>want open account</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "      <td>19</td>\n",
       "      <td>49</td>\n",
       "      <td>want open account</td>\n",
       "      <td>account open</td>\n",
       "      <td>account</td>\n",
       "      <td>want</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        Intent          Questions  target  clusters_1  \\\n",
       "368         368  Open Account  want open account      47          51   \n",
       "\n",
       "     clusters_2  clusters_3              lemma       keyword     noun  verb  \\\n",
       "368          19          49  want open account  account open  account  want   \n",
       "\n",
       "      adj  \n",
       "368  open  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.lemma == 'want open account']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross validation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf + linguistic + cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T03:10:57.422724Z",
     "start_time": "2020-04-02T03:10:35.447468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/alert/lib/python3.6/site-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
      "\n",
      "The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7863008720930232 0.013990250295576646\n",
      "0.8830062984496123 0.02030175344843629\n",
      "0.928234011627907 0.018760349023333362\n"
     ]
    }
   ],
   "source": [
    "# combine model score\n",
    "countvector_cols = ['lemma', 'keyword', 'noun', 'verb']\n",
    "top_clusters_cols = ['clusters_1', 'clusters_2', 'clusters_3']\n",
    "\n",
    "feature_cols = countvector_cols + top_clusters_cols\n",
    "\n",
    "# StratifiedKFold coss validation \n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(data[feature_cols], data['target'])\n",
    "print(skf)\n",
    "\n",
    "cv_scores_top1 = []\n",
    "cv_scores_top2 = []\n",
    "cv_scores_top3 = []\n",
    "\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in skf.split(data[feature_cols], data['target']):\n",
    "    # get train, test data for each chunk \n",
    "    X_train, X_test = data.loc[train_index,feature_cols], data.loc[test_index,feature_cols]\n",
    "    y_train, y_test = data.loc[train_index,'target'], data.loc[test_index,'target']\n",
    "    \n",
    "    v_lemma = TfidfVectorizer()\n",
    "    x_train_lemma = v_lemma.fit_transform(X_train['lemma'])\n",
    "    x_test_lemma = v_lemma.transform(X_test['lemma'])\n",
    "    vocab_lemma = dict(v_lemma.vocabulary_)\n",
    "    \n",
    "    v_keyword = TfidfVectorizer()\n",
    "    x_train_keyword = v_keyword.fit_transform(X_train['keyword'])\n",
    "    x_test_keyword = v_keyword.transform(X_test['keyword'])\n",
    "    vocab_keyword = dict(v_keyword.vocabulary_)\n",
    "    \n",
    "    v_noun = TfidfVectorizer()\n",
    "    x_train_noun = v_noun.fit_transform(X_train['noun'])\n",
    "    x_test_noun = v_noun.transform(X_test['noun'])\n",
    "    vocab_noun = dict(v_noun.vocabulary_)\n",
    "    \n",
    "    v_verb = TfidfVectorizer()\n",
    "    x_train_verb = v_verb.fit_transform(X_train['verb'])\n",
    "    x_test_verb = v_verb.transform(X_test['verb'])\n",
    "    vocab_verb = dict(v_verb.vocabulary_)\n",
    "    \n",
    "#     v_adj = TfidfVectorizer()\n",
    "#     x_train_adj = v_adj.fit_transform(X_train['adj'])\n",
    "#     x_test_adj = v_adj.transform(X_test['adj'])\n",
    "#     vocab_adj = dict(v_adj.vocabulary_)\n",
    "\n",
    "    # combine all features \n",
    "    x_train_combined = hstack((x_train_lemma,x_train_keyword,x_train_noun,x_train_verb,X_train[top_clusters_cols].values),format='csr')\n",
    "    x_train_combined_columns= v_lemma.get_feature_names()+v_keyword.get_feature_names()+v_noun.get_feature_names()+v_verb.get_feature_names()+top_clusters_cols\n",
    "    \n",
    "    x_test_combined = hstack((x_test_lemma,x_test_keyword,x_test_noun,x_test_verb,X_test[top_clusters_cols].values),format='csr')\n",
    "    x_test_combined_columns= v_lemma.get_feature_names()+v_keyword.get_feature_names()+v_noun.get_feature_names()+v_verb.get_feature_names()+top_clusters_cols\n",
    "   \n",
    "    x_train_combined = pd.DataFrame(x_train_combined.toarray())\n",
    "    x_train_combined.columns = x_train_combined_columns\n",
    "    \n",
    "    x_test_combined = pd.DataFrame(x_test_combined.toarray())\n",
    "    x_test_combined.columns = x_test_combined_columns\n",
    "    \n",
    "    # build classifier\n",
    "    clf = RandomForestClassifier(max_depth=50, n_estimators=1000)\n",
    "    clf.fit(x_train_combined, y_train)\n",
    "    \n",
    "    \n",
    "    probs = clf.predict_proba(x_test_combined)\n",
    "    best_3 = pd.DataFrame(np.argsort(probs, axis=1)[:,-3:],columns=['top3','top2','top1'])\n",
    "    best_3['top1'] = clf.classes_[best_3['top1']]\n",
    "    best_3['top2'] = clf.classes_[best_3['top2']]\n",
    "    best_3['top3'] = clf.classes_[best_3['top3']]\n",
    "    \n",
    "    result = pd.concat([best_3.reset_index(drop=True),pd.DataFrame(y_test).reset_index(drop=True), X_test[feature_cols].reset_index(drop=True)], axis=1)\n",
    "    final_result = pd.concat([final_result, result])\n",
    "    \n",
    "    score_1 = result[result['top1'] == result['target']].shape[0] / result.shape[0]\n",
    "    score_2 = result[(result['top1'] == result['target']) | (result['top2'] == result['target'])].shape[0] / result.shape[0]\n",
    "    score_3 = result[(result['top1'] == result['target']) | (result['top2'] == result['target'])| (result['top3'] == result['target'])].shape[0] / result.shape[0]\n",
    "    cv_scores_top1.append(score_1)\n",
    "    cv_scores_top2.append(score_2)\n",
    "    cv_scores_top3.append(score_3)\n",
    "    \n",
    "print(np.mean(np.array((cv_scores_top1))), np.std(np.array((cv_scores_top1))))\n",
    "print(np.mean(np.array((cv_scores_top2))), np.std(np.array((cv_scores_top2))))\n",
    "print(np.mean(np.array((cv_scores_top3))), np.std(np.array((cv_scores_top3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T02:59:50.150689Z",
     "start_time": "2020-04-02T02:59:50.125384Z"
    }
   },
   "outputs": [],
   "source": [
    "intent_cols = ['target','top1','top2','top3', 'clusters_1',\n",
    "       'clusters_2', 'clusters_3']\n",
    "for col in intent_cols:\n",
    "    final_result[col] = final_result[col].apply(lambda x: [intent for intent, index in intent2index.items() if index == x])\n",
    "    final_result[col] = final_result[col].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T02:59:50.581012Z",
     "start_time": "2020-04-02T02:59:50.532303Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top3</th>\n",
       "      <th>top2</th>\n",
       "      <th>top1</th>\n",
       "      <th>target</th>\n",
       "      <th>lemma</th>\n",
       "      <th>keyword</th>\n",
       "      <th>noun</th>\n",
       "      <th>verb</th>\n",
       "      <th>clusters_1</th>\n",
       "      <th>clusters_2</th>\n",
       "      <th>clusters_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Account Fraud</td>\n",
       "      <td>Cancel credit card transaction</td>\n",
       "      <td>Cancel Credit or Debit Card</td>\n",
       "      <td>Cancel credit card transaction</td>\n",
       "      <td>-pron- cancel transaction -pron- card</td>\n",
       "      <td>cancel card transaction</td>\n",
       "      <td>cancel card -pron- transaction</td>\n",
       "      <td></td>\n",
       "      <td>Cancel credit card transaction</td>\n",
       "      <td>Card Cancellation</td>\n",
       "      <td>Cancel Credit or Debit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>CRS Enquiries</td>\n",
       "      <td>Increase credit card limit</td>\n",
       "      <td>Give a compliment</td>\n",
       "      <td>Credit Refund</td>\n",
       "      <td>credit refund</td>\n",
       "      <td>refund credit</td>\n",
       "      <td>refund credit</td>\n",
       "      <td></td>\n",
       "      <td>Credit Refund</td>\n",
       "      <td>Rebates</td>\n",
       "      <td>Paying a cancelled credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>File a complaint</td>\n",
       "      <td>Give a compliment</td>\n",
       "      <td>Speak with a customer service officer</td>\n",
       "      <td>Account Fraud</td>\n",
       "      <td>-pron- scamme</td>\n",
       "      <td></td>\n",
       "      <td>-pron- scamme</td>\n",
       "      <td></td>\n",
       "      <td>Statement request</td>\n",
       "      <td>Cancel ATM Card</td>\n",
       "      <td>Cancel Credit or Debit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Account Fraud</td>\n",
       "      <td>Uplift suspension on accounts</td>\n",
       "      <td>Dormant Account Activation</td>\n",
       "      <td>Account Fraud</td>\n",
       "      <td>think scamme -pron- account</td>\n",
       "      <td>account</td>\n",
       "      <td>account -pron- scamme</td>\n",
       "      <td>think</td>\n",
       "      <td>Open NISP Account</td>\n",
       "      <td>Open Account</td>\n",
       "      <td>Uplift suspension on accounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>CRS Enquiries</td>\n",
       "      <td>Give a compliment</td>\n",
       "      <td>File a complaint</td>\n",
       "      <td>Give a compliment</td>\n",
       "      <td>staff patient</td>\n",
       "      <td></td>\n",
       "      <td>staff</td>\n",
       "      <td></td>\n",
       "      <td>Give a compliment</td>\n",
       "      <td>File a complaint</td>\n",
       "      <td>Speak with a customer service officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Dormant Account Activation</td>\n",
       "      <td>How to close my account</td>\n",
       "      <td>Uplift suspension on accounts</td>\n",
       "      <td>Request for account fee waiver</td>\n",
       "      <td>minimum balance need -pron- account</td>\n",
       "      <td>account</td>\n",
       "      <td>account balance -pron-</td>\n",
       "      <td>need</td>\n",
       "      <td>Request for account fee waiver</td>\n",
       "      <td>Increase credit card limit</td>\n",
       "      <td>Uplift suspension on accounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Dormant Account Activation</td>\n",
       "      <td>Open Account</td>\n",
       "      <td>Uplift suspension on accounts</td>\n",
       "      <td>Request for account fee waiver</td>\n",
       "      <td>account minimum balance</td>\n",
       "      <td>account</td>\n",
       "      <td>balance account</td>\n",
       "      <td></td>\n",
       "      <td>Request for account fee waiver</td>\n",
       "      <td>Increase credit card limit</td>\n",
       "      <td>Decrease credit card limit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Increase credit card limit</td>\n",
       "      <td>Credit card application rejection</td>\n",
       "      <td>Cancel Credit or Debit Card</td>\n",
       "      <td>Apply for credit or debit cards</td>\n",
       "      <td>high -pron- income credit card</td>\n",
       "      <td>card credit</td>\n",
       "      <td>income card -pron- credit</td>\n",
       "      <td></td>\n",
       "      <td>Increase credit card limit</td>\n",
       "      <td>Unsuccessful card transaction</td>\n",
       "      <td>Credit Refund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Debit card statement</td>\n",
       "      <td>Card statements</td>\n",
       "      <td>Credit card statement</td>\n",
       "      <td>Card dispute</td>\n",
       "      <td>extra charge -pron- credit card statement</td>\n",
       "      <td>statement card credit</td>\n",
       "      <td>credit charge card statement -pron-</td>\n",
       "      <td></td>\n",
       "      <td>Credit card statement</td>\n",
       "      <td>Card statements</td>\n",
       "      <td>Card dispute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Change credit card limit</td>\n",
       "      <td>Increase credit card limit</td>\n",
       "      <td>Decrease credit card limit</td>\n",
       "      <td>Increase credit card limit</td>\n",
       "      <td>increase card limit</td>\n",
       "      <td>limit card increase</td>\n",
       "      <td>limit card</td>\n",
       "      <td>increase</td>\n",
       "      <td>Decrease credit card limit</td>\n",
       "      <td>Increase credit card limit</td>\n",
       "      <td>Change credit card limit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           top3                               top2  \\\n",
       "25                Account Fraud     Cancel credit card transaction   \n",
       "27                CRS Enquiries         Increase credit card limit   \n",
       "31             File a complaint                  Give a compliment   \n",
       "32                Account Fraud      Uplift suspension on accounts   \n",
       "38                CRS Enquiries                  Give a compliment   \n",
       "..                          ...                                ...   \n",
       "100  Dormant Account Activation            How to close my account   \n",
       "101  Dormant Account Activation                       Open Account   \n",
       "108  Increase credit card limit  Credit card application rejection   \n",
       "114        Debit card statement                    Card statements   \n",
       "117    Change credit card limit         Increase credit card limit   \n",
       "\n",
       "                                      top1                           target  \\\n",
       "25             Cancel Credit or Debit Card   Cancel credit card transaction   \n",
       "27                       Give a compliment                    Credit Refund   \n",
       "31   Speak with a customer service officer                    Account Fraud   \n",
       "32              Dormant Account Activation                    Account Fraud   \n",
       "38                        File a complaint                Give a compliment   \n",
       "..                                     ...                              ...   \n",
       "100          Uplift suspension on accounts   Request for account fee waiver   \n",
       "101          Uplift suspension on accounts   Request for account fee waiver   \n",
       "108            Cancel Credit or Debit Card  Apply for credit or debit cards   \n",
       "114                  Credit card statement                     Card dispute   \n",
       "117             Decrease credit card limit       Increase credit card limit   \n",
       "\n",
       "                                         lemma                  keyword  \\\n",
       "25       -pron- cancel transaction -pron- card  cancel card transaction   \n",
       "27                               credit refund            refund credit   \n",
       "31                               -pron- scamme                            \n",
       "32                 think scamme -pron- account                  account   \n",
       "38                               staff patient                            \n",
       "..                                         ...                      ...   \n",
       "100        minimum balance need -pron- account                  account   \n",
       "101                    account minimum balance                  account   \n",
       "108             high -pron- income credit card              card credit   \n",
       "114  extra charge -pron- credit card statement    statement card credit   \n",
       "117                        increase card limit      limit card increase   \n",
       "\n",
       "                                    noun      verb  \\\n",
       "25        cancel card -pron- transaction             \n",
       "27                         refund credit             \n",
       "31                         -pron- scamme             \n",
       "32                 account -pron- scamme     think   \n",
       "38                                 staff             \n",
       "..                                   ...       ...   \n",
       "100               account balance -pron-      need   \n",
       "101                      balance account             \n",
       "108            income card -pron- credit             \n",
       "114  credit charge card statement -pron-             \n",
       "117                           limit card  increase   \n",
       "\n",
       "                         clusters_1                     clusters_2  \\\n",
       "25   Cancel credit card transaction              Card Cancellation   \n",
       "27                    Credit Refund                        Rebates   \n",
       "31                Statement request                Cancel ATM Card   \n",
       "32                Open NISP Account                   Open Account   \n",
       "38                Give a compliment               File a complaint   \n",
       "..                              ...                            ...   \n",
       "100  Request for account fee waiver     Increase credit card limit   \n",
       "101  Request for account fee waiver     Increase credit card limit   \n",
       "108      Increase credit card limit  Unsuccessful card transaction   \n",
       "114           Credit card statement                Card statements   \n",
       "117      Decrease credit card limit     Increase credit card limit   \n",
       "\n",
       "                                clusters_3  \n",
       "25             Cancel Credit or Debit Card  \n",
       "27          Paying a cancelled credit card  \n",
       "31             Cancel Credit or Debit Card  \n",
       "32           Uplift suspension on accounts  \n",
       "38   Speak with a customer service officer  \n",
       "..                                     ...  \n",
       "100          Uplift suspension on accounts  \n",
       "101             Decrease credit card limit  \n",
       "108                          Credit Refund  \n",
       "114                           Card dispute  \n",
       "117               Change credit card limit  \n",
       "\n",
       "[137 rows x 11 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result[final_result['target'] != final_result['top1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf + linguistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T03:11:44.287648Z",
     "start_time": "2020-04-02T03:11:26.779449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/alert/lib/python3.6/site-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
      "\n",
      "The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764389534883721 0.018545295601492245\n",
      "0.8814559108527131 0.01859948532579604\n",
      "0.9188832364341085 0.015270786365466856\n"
     ]
    }
   ],
   "source": [
    "# combine model score\n",
    "countvector_cols = ['lemma', 'keyword', 'noun', 'verb']\n",
    "top_clusters_cols = ['clusters_1', 'clusters_2', 'clusters_3']\n",
    "\n",
    "feature_cols = countvector_cols + top_clusters_cols\n",
    "\n",
    "# StratifiedKFold coss validation \n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(data[feature_cols], data['target'])\n",
    "print(skf)\n",
    "\n",
    "cv_scores_top1 = []\n",
    "cv_scores_top2 = []\n",
    "cv_scores_top3 = []\n",
    "\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in skf.split(data[feature_cols], data['target']):\n",
    "    # get train, test data for each chunk \n",
    "    X_train, X_test = data.loc[train_index,feature_cols], data.loc[test_index,feature_cols]\n",
    "    y_train, y_test = data.loc[train_index,'target'], data.loc[test_index,'target']\n",
    "    \n",
    "    v_lemma = TfidfVectorizer()\n",
    "    x_train_lemma = v_lemma.fit_transform(X_train['lemma'])\n",
    "    x_test_lemma = v_lemma.transform(X_test['lemma'])\n",
    "    vocab_lemma = dict(v_lemma.vocabulary_)\n",
    "    \n",
    "    v_keyword = TfidfVectorizer()\n",
    "    x_train_keyword = v_keyword.fit_transform(X_train['keyword'])\n",
    "    x_test_keyword = v_keyword.transform(X_test['keyword'])\n",
    "    vocab_keyword = dict(v_keyword.vocabulary_)\n",
    "    \n",
    "    v_noun = TfidfVectorizer()\n",
    "    x_train_noun = v_noun.fit_transform(X_train['noun'])\n",
    "    x_test_noun = v_noun.transform(X_test['noun'])\n",
    "    vocab_noun = dict(v_noun.vocabulary_)\n",
    "    \n",
    "    v_verb = TfidfVectorizer()\n",
    "    x_train_verb = v_verb.fit_transform(X_train['verb'])\n",
    "    x_test_verb = v_verb.transform(X_test['verb'])\n",
    "    vocab_verb = dict(v_verb.vocabulary_)\n",
    "\n",
    "    # combine all features \n",
    "    x_train_combined = hstack((x_train_lemma,x_train_keyword,x_train_noun,x_train_verb),format='csr')\n",
    "    x_train_combined_columns= v_lemma.get_feature_names()+v_keyword.get_feature_names()+v_noun.get_feature_names()+v_verb.get_feature_names()\n",
    "    \n",
    "    x_test_combined = hstack((x_test_lemma,x_test_keyword,x_test_noun,x_test_verb),format='csr')\n",
    "    x_test_combined_columns= v_lemma.get_feature_names()+v_keyword.get_feature_names()+v_noun.get_feature_names()+v_verb.get_feature_names()\n",
    "   \n",
    "    \n",
    "    x_train_combined = pd.DataFrame(x_train_combined.toarray())\n",
    "    x_train_combined.columns = x_train_combined_columns\n",
    "    \n",
    "    x_test_combined = pd.DataFrame(x_test_combined.toarray())\n",
    "    x_test_combined.columns = x_test_combined_columns\n",
    "    \n",
    "    # build classifier\n",
    "    clf = RandomForestClassifier(max_depth=50, n_estimators=800)\n",
    "    clf.fit(x_train_combined, y_train)\n",
    "    \n",
    "    \n",
    "    probs = clf.predict_proba(x_test_combined)\n",
    "    best_3 = pd.DataFrame(np.argsort(probs, axis=1)[:,-3:],columns=['top3','top2','top1'])\n",
    "    best_3['top1'] = clf.classes_[best_3['top1']]\n",
    "    best_3['top2'] = clf.classes_[best_3['top2']]\n",
    "    best_3['top3'] = clf.classes_[best_3['top3']]\n",
    "    \n",
    "    result = pd.concat([best_3.reset_index(drop=True),pd.DataFrame(y_test).reset_index(drop=True), X_test[feature_cols].reset_index(drop=True)], axis=1)\n",
    "    final_result = pd.concat([final_result, result])\n",
    "    \n",
    "    score_1 = result[result['top1'] == result['target']].shape[0] / result.shape[0]\n",
    "    score_2 = result[(result['top1'] == result['target']) | (result['top2'] == result['target'])].shape[0] / result.shape[0]\n",
    "    score_3 = result[(result['top1'] == result['target']) | (result['top2'] == result['target'])| (result['top3'] == result['target'])].shape[0] / result.shape[0]\n",
    "    cv_scores_top1.append(score_1)\n",
    "    cv_scores_top2.append(score_2)\n",
    "    cv_scores_top3.append(score_3)\n",
    "    \n",
    "print(np.mean(np.array((cv_scores_top1))), np.std(np.array((cv_scores_top1))))\n",
    "print(np.mean(np.array((cv_scores_top2))), np.std(np.array((cv_scores_top2))))\n",
    "print(np.mean(np.array((cv_scores_top3))), np.std(np.array((cv_scores_top3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave one out cross validation result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf + linguistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T05:41:55.601462Z",
     "start_time": "2020-04-02T03:13:31.451440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeaveOneOut()\n",
      "TEST: [0]\n",
      "TEST: [1]\n",
      "TEST: [2]\n",
      "TEST: [3]\n",
      "TEST: [4]\n",
      "TEST: [5]\n",
      "TEST: [6]\n",
      "TEST: [7]\n",
      "TEST: [8]\n",
      "TEST: [9]\n",
      "TEST: [10]\n",
      "TEST: [11]\n",
      "TEST: [12]\n",
      "TEST: [13]\n",
      "TEST: [14]\n",
      "TEST: [15]\n",
      "TEST: [16]\n",
      "TEST: [17]\n",
      "TEST: [18]\n",
      "TEST: [19]\n",
      "TEST: [20]\n",
      "TEST: [21]\n",
      "TEST: [22]\n",
      "TEST: [23]\n",
      "TEST: [24]\n",
      "TEST: [25]\n",
      "TEST: [26]\n",
      "TEST: [27]\n",
      "TEST: [28]\n",
      "TEST: [29]\n",
      "TEST: [30]\n",
      "TEST: [31]\n",
      "TEST: [32]\n",
      "TEST: [33]\n",
      "TEST: [34]\n",
      "TEST: [35]\n",
      "TEST: [36]\n",
      "TEST: [37]\n",
      "TEST: [38]\n",
      "TEST: [39]\n",
      "TEST: [40]\n",
      "TEST: [41]\n",
      "TEST: [42]\n",
      "TEST: [43]\n",
      "TEST: [44]\n",
      "TEST: [45]\n",
      "TEST: [46]\n",
      "TEST: [47]\n",
      "TEST: [48]\n",
      "TEST: [49]\n",
      "TEST: [50]\n",
      "TEST: [51]\n",
      "TEST: [52]\n",
      "TEST: [53]\n",
      "TEST: [54]\n",
      "TEST: [55]\n",
      "TEST: [56]\n",
      "TEST: [57]\n",
      "TEST: [58]\n",
      "TEST: [59]\n",
      "TEST: [60]\n",
      "TEST: [61]\n",
      "TEST: [62]\n",
      "TEST: [63]\n",
      "TEST: [64]\n",
      "TEST: [65]\n",
      "TEST: [66]\n",
      "TEST: [67]\n",
      "TEST: [68]\n",
      "TEST: [69]\n",
      "TEST: [70]\n",
      "TEST: [71]\n",
      "TEST: [72]\n",
      "TEST: [73]\n",
      "TEST: [74]\n",
      "TEST: [75]\n",
      "TEST: [76]\n",
      "TEST: [77]\n",
      "TEST: [78]\n",
      "TEST: [79]\n",
      "TEST: [80]\n",
      "TEST: [81]\n",
      "TEST: [82]\n",
      "TEST: [83]\n",
      "TEST: [84]\n",
      "TEST: [85]\n",
      "TEST: [86]\n",
      "TEST: [87]\n",
      "TEST: [88]\n",
      "TEST: [89]\n",
      "TEST: [90]\n",
      "TEST: [91]\n",
      "TEST: [92]\n",
      "TEST: [93]\n",
      "TEST: [94]\n",
      "TEST: [95]\n",
      "TEST: [96]\n",
      "TEST: [97]\n",
      "TEST: [98]\n",
      "TEST: [99]\n",
      "TEST: [100]\n",
      "TEST: [101]\n",
      "TEST: [102]\n",
      "TEST: [103]\n",
      "TEST: [104]\n",
      "TEST: [105]\n",
      "TEST: [106]\n",
      "TEST: [107]\n",
      "TEST: [108]\n",
      "TEST: [109]\n",
      "TEST: [110]\n",
      "TEST: [111]\n",
      "TEST: [112]\n",
      "TEST: [113]\n",
      "TEST: [114]\n",
      "TEST: [115]\n",
      "TEST: [116]\n",
      "TEST: [117]\n",
      "TEST: [118]\n",
      "TEST: [119]\n",
      "TEST: [120]\n",
      "TEST: [121]\n",
      "TEST: [122]\n",
      "TEST: [123]\n",
      "TEST: [124]\n",
      "TEST: [125]\n",
      "TEST: [126]\n",
      "TEST: [127]\n",
      "TEST: [128]\n",
      "TEST: [129]\n",
      "TEST: [130]\n",
      "TEST: [131]\n",
      "TEST: [132]\n",
      "TEST: [133]\n",
      "TEST: [134]\n",
      "TEST: [135]\n",
      "TEST: [136]\n",
      "TEST: [137]\n",
      "TEST: [138]\n",
      "TEST: [139]\n",
      "TEST: [140]\n",
      "TEST: [141]\n",
      "TEST: [142]\n",
      "TEST: [143]\n",
      "TEST: [144]\n",
      "TEST: [145]\n",
      "TEST: [146]\n",
      "TEST: [147]\n",
      "TEST: [148]\n",
      "TEST: [149]\n",
      "TEST: [150]\n",
      "TEST: [151]\n",
      "TEST: [152]\n",
      "TEST: [153]\n",
      "TEST: [154]\n",
      "TEST: [155]\n",
      "TEST: [156]\n",
      "TEST: [157]\n",
      "TEST: [158]\n",
      "TEST: [159]\n",
      "TEST: [160]\n",
      "TEST: [161]\n",
      "TEST: [162]\n",
      "TEST: [163]\n",
      "TEST: [164]\n",
      "TEST: [165]\n",
      "TEST: [166]\n",
      "TEST: [167]\n",
      "TEST: [168]\n",
      "TEST: [169]\n",
      "TEST: [170]\n",
      "TEST: [171]\n",
      "TEST: [172]\n",
      "TEST: [173]\n",
      "TEST: [174]\n",
      "TEST: [175]\n",
      "TEST: [176]\n",
      "TEST: [177]\n",
      "TEST: [178]\n",
      "TEST: [179]\n",
      "TEST: [180]\n",
      "TEST: [181]\n",
      "TEST: [182]\n",
      "TEST: [183]\n",
      "TEST: [184]\n",
      "TEST: [185]\n",
      "TEST: [186]\n",
      "TEST: [187]\n",
      "TEST: [188]\n",
      "TEST: [189]\n",
      "TEST: [190]\n",
      "TEST: [191]\n",
      "TEST: [192]\n",
      "TEST: [193]\n",
      "TEST: [194]\n",
      "TEST: [195]\n",
      "TEST: [196]\n",
      "TEST: [197]\n",
      "TEST: [198]\n",
      "TEST: [199]\n",
      "TEST: [200]\n",
      "TEST: [201]\n",
      "TEST: [202]\n",
      "TEST: [203]\n",
      "TEST: [204]\n",
      "TEST: [205]\n",
      "TEST: [206]\n",
      "TEST: [207]\n",
      "TEST: [208]\n",
      "TEST: [209]\n",
      "TEST: [210]\n",
      "TEST: [211]\n",
      "TEST: [212]\n",
      "TEST: [213]\n",
      "TEST: [214]\n",
      "TEST: [215]\n",
      "TEST: [216]\n",
      "TEST: [217]\n",
      "TEST: [218]\n",
      "TEST: [219]\n",
      "TEST: [220]\n",
      "TEST: [221]\n",
      "TEST: [222]\n",
      "TEST: [223]\n",
      "TEST: [224]\n",
      "TEST: [225]\n",
      "TEST: [226]\n",
      "TEST: [227]\n",
      "TEST: [228]\n",
      "TEST: [229]\n",
      "TEST: [230]\n",
      "TEST: [231]\n",
      "TEST: [232]\n",
      "TEST: [233]\n",
      "TEST: [234]\n",
      "TEST: [235]\n",
      "TEST: [236]\n",
      "TEST: [237]\n",
      "TEST: [238]\n",
      "TEST: [239]\n",
      "TEST: [240]\n",
      "TEST: [241]\n",
      "TEST: [242]\n",
      "TEST: [243]\n",
      "TEST: [244]\n",
      "TEST: [245]\n",
      "TEST: [246]\n",
      "TEST: [247]\n",
      "TEST: [248]\n",
      "TEST: [249]\n",
      "TEST: [250]\n",
      "TEST: [251]\n",
      "TEST: [252]\n",
      "TEST: [253]\n",
      "TEST: [254]\n",
      "TEST: [255]\n",
      "TEST: [256]\n",
      "TEST: [257]\n",
      "TEST: [258]\n",
      "TEST: [259]\n",
      "TEST: [260]\n",
      "TEST: [261]\n",
      "TEST: [262]\n",
      "TEST: [263]\n",
      "TEST: [264]\n",
      "TEST: [265]\n",
      "TEST: [266]\n",
      "TEST: [267]\n",
      "TEST: [268]\n",
      "TEST: [269]\n",
      "TEST: [270]\n",
      "TEST: [271]\n",
      "TEST: [272]\n",
      "TEST: [273]\n",
      "TEST: [274]\n",
      "TEST: [275]\n",
      "TEST: [276]\n",
      "TEST: [277]\n",
      "TEST: [278]\n",
      "TEST: [279]\n",
      "TEST: [280]\n",
      "TEST: [281]\n",
      "TEST: [282]\n",
      "TEST: [283]\n",
      "TEST: [284]\n",
      "TEST: [285]\n",
      "TEST: [286]\n",
      "TEST: [287]\n",
      "TEST: [288]\n",
      "TEST: [289]\n",
      "TEST: [290]\n",
      "TEST: [291]\n",
      "TEST: [292]\n",
      "TEST: [293]\n",
      "TEST: [294]\n",
      "TEST: [295]\n",
      "TEST: [296]\n",
      "TEST: [297]\n",
      "TEST: [298]\n",
      "TEST: [299]\n",
      "TEST: [300]\n",
      "TEST: [301]\n",
      "TEST: [302]\n",
      "TEST: [303]\n",
      "TEST: [304]\n",
      "TEST: [305]\n",
      "TEST: [306]\n",
      "TEST: [307]\n",
      "TEST: [308]\n",
      "TEST: [309]\n",
      "TEST: [310]\n",
      "TEST: [311]\n",
      "TEST: [312]\n",
      "TEST: [313]\n",
      "TEST: [314]\n",
      "TEST: [315]\n",
      "TEST: [316]\n",
      "TEST: [317]\n",
      "TEST: [318]\n",
      "TEST: [319]\n",
      "TEST: [320]\n",
      "TEST: [321]\n",
      "TEST: [322]\n",
      "TEST: [323]\n",
      "TEST: [324]\n",
      "TEST: [325]\n",
      "TEST: [326]\n",
      "TEST: [327]\n",
      "TEST: [328]\n",
      "TEST: [329]\n",
      "TEST: [330]\n",
      "TEST: [331]\n",
      "TEST: [332]\n",
      "TEST: [333]\n",
      "TEST: [334]\n",
      "TEST: [335]\n",
      "TEST: [336]\n",
      "TEST: [337]\n",
      "TEST: [338]\n",
      "TEST: [339]\n",
      "TEST: [340]\n",
      "TEST: [341]\n",
      "TEST: [342]\n",
      "TEST: [343]\n",
      "TEST: [344]\n",
      "TEST: [345]\n",
      "TEST: [346]\n",
      "TEST: [347]\n",
      "TEST: [348]\n",
      "TEST: [349]\n",
      "TEST: [350]\n",
      "TEST: [351]\n",
      "TEST: [352]\n",
      "TEST: [353]\n",
      "TEST: [354]\n",
      "TEST: [355]\n",
      "TEST: [356]\n",
      "TEST: [357]\n",
      "TEST: [358]\n",
      "TEST: [359]\n",
      "TEST: [360]\n",
      "TEST: [361]\n",
      "TEST: [362]\n",
      "TEST: [363]\n",
      "TEST: [364]\n",
      "TEST: [365]\n",
      "TEST: [366]\n",
      "TEST: [367]\n",
      "TEST: [368]\n",
      "TEST: [369]\n",
      "TEST: [370]\n",
      "TEST: [371]\n",
      "TEST: [372]\n",
      "TEST: [373]\n",
      "TEST: [374]\n",
      "TEST: [375]\n",
      "TEST: [376]\n",
      "TEST: [377]\n",
      "TEST: [378]\n",
      "TEST: [379]\n",
      "TEST: [380]\n",
      "TEST: [381]\n",
      "TEST: [382]\n",
      "TEST: [383]\n",
      "TEST: [384]\n",
      "TEST: [385]\n",
      "TEST: [386]\n",
      "TEST: [387]\n",
      "TEST: [388]\n",
      "TEST: [389]\n",
      "TEST: [390]\n",
      "TEST: [391]\n",
      "TEST: [392]\n",
      "TEST: [393]\n",
      "TEST: [394]\n",
      "TEST: [395]\n",
      "TEST: [396]\n",
      "TEST: [397]\n",
      "TEST: [398]\n",
      "TEST: [399]\n",
      "TEST: [400]\n",
      "TEST: [401]\n",
      "TEST: [402]\n",
      "TEST: [403]\n",
      "TEST: [404]\n",
      "TEST: [405]\n",
      "TEST: [406]\n",
      "TEST: [407]\n",
      "TEST: [408]\n",
      "TEST: [409]\n",
      "TEST: [410]\n",
      "TEST: [411]\n",
      "TEST: [412]\n",
      "TEST: [413]\n",
      "TEST: [414]\n",
      "TEST: [415]\n",
      "TEST: [416]\n",
      "TEST: [417]\n",
      "TEST: [418]\n",
      "TEST: [419]\n",
      "TEST: [420]\n",
      "TEST: [421]\n",
      "TEST: [422]\n",
      "TEST: [423]\n",
      "TEST: [424]\n",
      "TEST: [425]\n",
      "TEST: [426]\n",
      "TEST: [427]\n",
      "TEST: [428]\n",
      "TEST: [429]\n",
      "TEST: [430]\n",
      "TEST: [431]\n",
      "TEST: [432]\n",
      "TEST: [433]\n",
      "TEST: [434]\n",
      "TEST: [435]\n",
      "TEST: [436]\n",
      "TEST: [437]\n",
      "TEST: [438]\n",
      "TEST: [439]\n",
      "TEST: [440]\n",
      "TEST: [441]\n",
      "TEST: [442]\n",
      "TEST: [443]\n",
      "TEST: [444]\n",
      "TEST: [445]\n",
      "TEST: [446]\n",
      "TEST: [447]\n",
      "TEST: [448]\n",
      "TEST: [449]\n",
      "TEST: [450]\n",
      "TEST: [451]\n",
      "TEST: [452]\n",
      "TEST: [453]\n",
      "TEST: [454]\n",
      "TEST: [455]\n",
      "TEST: [456]\n",
      "TEST: [457]\n",
      "TEST: [458]\n",
      "TEST: [459]\n",
      "TEST: [460]\n",
      "TEST: [461]\n",
      "TEST: [462]\n",
      "TEST: [463]\n",
      "TEST: [464]\n",
      "TEST: [465]\n",
      "TEST: [466]\n",
      "TEST: [467]\n",
      "TEST: [468]\n",
      "TEST: [469]\n",
      "TEST: [470]\n",
      "TEST: [471]\n",
      "TEST: [472]\n",
      "TEST: [473]\n",
      "TEST: [474]\n",
      "TEST: [475]\n",
      "TEST: [476]\n",
      "TEST: [477]\n",
      "TEST: [478]\n",
      "TEST: [479]\n",
      "TEST: [480]\n",
      "TEST: [481]\n",
      "TEST: [482]\n",
      "TEST: [483]\n",
      "TEST: [484]\n",
      "TEST: [485]\n",
      "TEST: [486]\n",
      "TEST: [487]\n",
      "TEST: [488]\n",
      "TEST: [489]\n",
      "TEST: [490]\n",
      "TEST: [491]\n",
      "TEST: [492]\n",
      "TEST: [493]\n",
      "TEST: [494]\n",
      "TEST: [495]\n",
      "TEST: [496]\n",
      "TEST: [497]\n",
      "TEST: [498]\n",
      "TEST: [499]\n",
      "TEST: [500]\n",
      "TEST: [501]\n",
      "TEST: [502]\n",
      "TEST: [503]\n",
      "TEST: [504]\n",
      "TEST: [505]\n",
      "TEST: [506]\n",
      "TEST: [507]\n",
      "TEST: [508]\n",
      "TEST: [509]\n",
      "TEST: [510]\n",
      "TEST: [511]\n",
      "TEST: [512]\n",
      "TEST: [513]\n",
      "TEST: [514]\n",
      "TEST: [515]\n",
      "TEST: [516]\n",
      "TEST: [517]\n",
      "TEST: [518]\n",
      "TEST: [519]\n",
      "TEST: [520]\n",
      "TEST: [521]\n",
      "TEST: [522]\n",
      "TEST: [523]\n",
      "TEST: [524]\n",
      "TEST: [525]\n",
      "TEST: [526]\n",
      "TEST: [527]\n",
      "TEST: [528]\n",
      "TEST: [529]\n",
      "TEST: [530]\n",
      "TEST: [531]\n",
      "TEST: [532]\n",
      "TEST: [533]\n",
      "TEST: [534]\n",
      "TEST: [535]\n",
      "TEST: [536]\n",
      "TEST: [537]\n",
      "TEST: [538]\n",
      "TEST: [539]\n",
      "TEST: [540]\n",
      "TEST: [541]\n",
      "TEST: [542]\n",
      "TEST: [543]\n",
      "TEST: [544]\n",
      "TEST: [545]\n",
      "TEST: [546]\n",
      "TEST: [547]\n",
      "TEST: [548]\n",
      "TEST: [549]\n",
      "TEST: [550]\n",
      "TEST: [551]\n",
      "TEST: [552]\n",
      "TEST: [553]\n",
      "TEST: [554]\n",
      "TEST: [555]\n",
      "TEST: [556]\n",
      "TEST: [557]\n",
      "TEST: [558]\n",
      "TEST: [559]\n",
      "TEST: [560]\n",
      "TEST: [561]\n",
      "TEST: [562]\n",
      "TEST: [563]\n",
      "TEST: [564]\n",
      "TEST: [565]\n",
      "TEST: [566]\n",
      "TEST: [567]\n",
      "TEST: [568]\n",
      "TEST: [569]\n",
      "TEST: [570]\n",
      "TEST: [571]\n",
      "TEST: [572]\n",
      "TEST: [573]\n",
      "TEST: [574]\n",
      "TEST: [575]\n",
      "TEST: [576]\n",
      "TEST: [577]\n",
      "TEST: [578]\n",
      "TEST: [579]\n",
      "TEST: [580]\n",
      "TEST: [581]\n",
      "TEST: [582]\n",
      "TEST: [583]\n",
      "TEST: [584]\n",
      "TEST: [585]\n",
      "TEST: [586]\n",
      "TEST: [587]\n",
      "TEST: [588]\n",
      "TEST: [589]\n",
      "TEST: [590]\n",
      "TEST: [591]\n",
      "TEST: [592]\n",
      "TEST: [593]\n",
      "TEST: [594]\n",
      "TEST: [595]\n",
      "TEST: [596]\n",
      "TEST: [597]\n",
      "TEST: [598]\n",
      "TEST: [599]\n",
      "TEST: [600]\n",
      "TEST: [601]\n",
      "TEST: [602]\n",
      "TEST: [603]\n",
      "TEST: [604]\n",
      "TEST: [605]\n",
      "TEST: [606]\n",
      "TEST: [607]\n",
      "TEST: [608]\n",
      "TEST: [609]\n",
      "TEST: [610]\n",
      "TEST: [611]\n",
      "TEST: [612]\n",
      "TEST: [613]\n",
      "TEST: [614]\n",
      "TEST: [615]\n",
      "TEST: [616]\n",
      "TEST: [617]\n",
      "TEST: [618]\n",
      "TEST: [619]\n",
      "TEST: [620]\n",
      "TEST: [621]\n",
      "TEST: [622]\n",
      "TEST: [623]\n",
      "TEST: [624]\n",
      "TEST: [625]\n",
      "TEST: [626]\n",
      "TEST: [627]\n",
      "TEST: [628]\n",
      "TEST: [629]\n",
      "TEST: [630]\n",
      "TEST: [631]\n",
      "TEST: [632]\n",
      "TEST: [633]\n",
      "TEST: [634]\n",
      "TEST: [635]\n",
      "TEST: [636]\n",
      "TEST: [637]\n",
      "TEST: [638]\n",
      "TEST: [639]\n",
      "TEST: [640]\n",
      "0.7722308892355694 0.41939282653141685\n",
      "0.890795631825273 0.31189545387242446\n",
      "0.9344773790951638 0.24744576588536996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "# combine model score\n",
    "countvector_cols = ['lemma', 'keyword', 'noun', 'verb']\n",
    "top_clusters_cols = ['clusters_1', 'clusters_2', 'clusters_3']\n",
    "\n",
    "feature_cols = countvector_cols + top_clusters_cols\n",
    "\n",
    "# LeaveOneOut coss validation \n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(data[feature_cols], data['target'])\n",
    "\n",
    "print(loo)\n",
    "\n",
    "cv_scores_top1 = []\n",
    "cv_scores_top2 = []\n",
    "cv_scores_top3 = []\n",
    "\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in loo.split(data[feature_cols], data['target']):\n",
    "    # print(\"TEST:\", test_index)\n",
    "    # get train, test data for each chunk \n",
    "    X_train, X_test = data.loc[train_index,feature_cols], data.loc[test_index,feature_cols]\n",
    "    y_train, y_test = data.loc[train_index,'target'], data.loc[test_index,'target']\n",
    "    \n",
    "    v_lemma = TfidfVectorizer()\n",
    "    x_train_lemma = v_lemma.fit_transform(X_train['lemma'])\n",
    "    x_test_lemma = v_lemma.transform(X_test['lemma'])\n",
    "    vocab_lemma = dict(v_lemma.vocabulary_)\n",
    "    \n",
    "    v_keyword = TfidfVectorizer()\n",
    "    x_train_keyword = v_keyword.fit_transform(X_train['keyword'])\n",
    "    x_test_keyword = v_keyword.transform(X_test['keyword'])\n",
    "    vocab_keyword = dict(v_keyword.vocabulary_)\n",
    "    \n",
    "    v_noun = TfidfVectorizer()\n",
    "    x_train_noun = v_noun.fit_transform(X_train['noun'])\n",
    "    x_test_noun = v_noun.transform(X_test['noun'])\n",
    "    vocab_noun = dict(v_noun.vocabulary_)\n",
    "    \n",
    "    v_verb = TfidfVectorizer()\n",
    "    x_train_verb = v_verb.fit_transform(X_train['verb'])\n",
    "    x_test_verb = v_verb.transform(X_test['verb'])\n",
    "    vocab_verb = dict(v_verb.vocabulary_)\n",
    "\n",
    "    # combine all features \n",
    "    x_train_combined = hstack((x_train_lemma,x_train_keyword,x_train_noun,x_train_verb),format='csr')\n",
    "    x_train_combined_columns= v_lemma.get_feature_names()+v_keyword.get_feature_names()+v_noun.get_feature_names()+v_verb.get_feature_names()\n",
    "    \n",
    "    x_test_combined = hstack((x_test_lemma,x_test_keyword,x_test_noun,x_test_verb),format='csr')\n",
    "    x_test_combined_columns= v_lemma.get_feature_names()+v_keyword.get_feature_names()+v_noun.get_feature_names()+v_verb.get_feature_names()\n",
    "   \n",
    "    \n",
    "    x_train_combined = pd.DataFrame(x_train_combined.toarray())\n",
    "    x_train_combined.columns = x_train_combined_columns\n",
    "    \n",
    "    x_test_combined = pd.DataFrame(x_test_combined.toarray())\n",
    "    x_test_combined.columns = x_test_combined_columns\n",
    "    \n",
    "    # build classifier\n",
    "    clf = RandomForestClassifier(max_depth=50, n_estimators=1000)\n",
    "    clf.fit(x_train_combined, y_train)\n",
    "    \n",
    "    \n",
    "    probs = clf.predict_proba(x_test_combined)\n",
    "    best_3 = pd.DataFrame(np.argsort(probs, axis=1)[:,-3:],columns=['top3','top2','top1'])\n",
    "    best_3['top1'] = clf.classes_[best_3['top1']]\n",
    "    best_3['top2'] = clf.classes_[best_3['top2']]\n",
    "    best_3['top3'] = clf.classes_[best_3['top3']]\n",
    "    \n",
    "    result = pd.concat([best_3.reset_index(drop=True),pd.DataFrame(y_test).reset_index(drop=True), X_test[feature_cols].reset_index(drop=True)], axis=1)\n",
    "    final_result = pd.concat([final_result, result])\n",
    "    \n",
    "    score_1 = result[result['top1'] == result['target']].shape[0] / result.shape[0]\n",
    "    score_2 = result[(result['top1'] == result['target']) | (result['top2'] == result['target'])].shape[0] / result.shape[0]\n",
    "    score_3 = result[(result['top1'] == result['target']) | (result['top2'] == result['target'])| (result['top3'] == result['target'])].shape[0] / result.shape[0]\n",
    "    cv_scores_top1.append(score_1)\n",
    "    cv_scores_top2.append(score_2)\n",
    "    cv_scores_top3.append(score_3)\n",
    "    \n",
    "print(np.mean(np.array((cv_scores_top1))), np.std(np.array((cv_scores_top1))))\n",
    "print(np.mean(np.array((cv_scores_top2))), np.std(np.array((cv_scores_top2))))\n",
    "print(np.mean(np.array((cv_scores_top3))), np.std(np.array((cv_scores_top3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf_idf + linguistic + cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T06:35:28.536154Z",
     "start_time": "2020-04-02T05:51:08.448488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeaveOneOut()\n",
      "TEST: [0]\n",
      "TEST: [1]\n",
      "TEST: [2]\n",
      "TEST: [3]\n",
      "TEST: [4]\n",
      "TEST: [5]\n",
      "TEST: [6]\n",
      "TEST: [7]\n",
      "TEST: [8]\n",
      "TEST: [9]\n",
      "TEST: [10]\n",
      "TEST: [11]\n",
      "TEST: [12]\n",
      "TEST: [13]\n",
      "TEST: [14]\n",
      "TEST: [15]\n",
      "TEST: [16]\n",
      "TEST: [17]\n",
      "TEST: [18]\n",
      "TEST: [19]\n",
      "TEST: [20]\n",
      "TEST: [21]\n",
      "TEST: [22]\n",
      "TEST: [23]\n",
      "TEST: [24]\n",
      "TEST: [25]\n",
      "TEST: [26]\n",
      "TEST: [27]\n",
      "TEST: [28]\n",
      "TEST: [29]\n",
      "TEST: [30]\n",
      "TEST: [31]\n",
      "TEST: [32]\n",
      "TEST: [33]\n",
      "TEST: [34]\n",
      "TEST: [35]\n",
      "TEST: [36]\n",
      "TEST: [37]\n",
      "TEST: [38]\n",
      "TEST: [39]\n",
      "TEST: [40]\n",
      "TEST: [41]\n",
      "TEST: [42]\n",
      "TEST: [43]\n",
      "TEST: [44]\n",
      "TEST: [45]\n",
      "TEST: [46]\n",
      "TEST: [47]\n",
      "TEST: [48]\n",
      "TEST: [49]\n",
      "TEST: [50]\n",
      "TEST: [51]\n",
      "TEST: [52]\n",
      "TEST: [53]\n",
      "TEST: [54]\n",
      "TEST: [55]\n",
      "TEST: [56]\n",
      "TEST: [57]\n",
      "TEST: [58]\n",
      "TEST: [59]\n",
      "TEST: [60]\n",
      "TEST: [61]\n",
      "TEST: [62]\n",
      "TEST: [63]\n",
      "TEST: [64]\n",
      "TEST: [65]\n",
      "TEST: [66]\n",
      "TEST: [67]\n",
      "TEST: [68]\n",
      "TEST: [69]\n",
      "TEST: [70]\n",
      "TEST: [71]\n",
      "TEST: [72]\n",
      "TEST: [73]\n",
      "TEST: [74]\n",
      "TEST: [75]\n",
      "TEST: [76]\n",
      "TEST: [77]\n",
      "TEST: [78]\n",
      "TEST: [79]\n",
      "TEST: [80]\n",
      "TEST: [81]\n",
      "TEST: [82]\n",
      "TEST: [83]\n",
      "TEST: [84]\n",
      "TEST: [85]\n",
      "TEST: [86]\n",
      "TEST: [87]\n",
      "TEST: [88]\n",
      "TEST: [89]\n",
      "TEST: [90]\n",
      "TEST: [91]\n",
      "TEST: [92]\n",
      "TEST: [93]\n",
      "TEST: [94]\n",
      "TEST: [95]\n",
      "TEST: [96]\n",
      "TEST: [97]\n",
      "TEST: [98]\n",
      "TEST: [99]\n",
      "TEST: [100]\n",
      "TEST: [101]\n",
      "TEST: [102]\n",
      "TEST: [103]\n",
      "TEST: [104]\n",
      "TEST: [105]\n",
      "TEST: [106]\n",
      "TEST: [107]\n",
      "TEST: [108]\n",
      "TEST: [109]\n",
      "TEST: [110]\n",
      "TEST: [111]\n",
      "TEST: [112]\n",
      "TEST: [113]\n",
      "TEST: [114]\n",
      "TEST: [115]\n",
      "TEST: [116]\n",
      "TEST: [117]\n",
      "TEST: [118]\n",
      "TEST: [119]\n",
      "TEST: [120]\n",
      "TEST: [121]\n",
      "TEST: [122]\n",
      "TEST: [123]\n",
      "TEST: [124]\n",
      "TEST: [125]\n",
      "TEST: [126]\n",
      "TEST: [127]\n",
      "TEST: [128]\n",
      "TEST: [129]\n",
      "TEST: [130]\n",
      "TEST: [131]\n",
      "TEST: [132]\n",
      "TEST: [133]\n",
      "TEST: [134]\n",
      "TEST: [135]\n",
      "TEST: [136]\n",
      "TEST: [137]\n",
      "TEST: [138]\n",
      "TEST: [139]\n",
      "TEST: [140]\n",
      "TEST: [141]\n",
      "TEST: [142]\n",
      "TEST: [143]\n",
      "TEST: [144]\n",
      "TEST: [145]\n",
      "TEST: [146]\n",
      "TEST: [147]\n",
      "TEST: [148]\n",
      "TEST: [149]\n",
      "TEST: [150]\n",
      "TEST: [151]\n",
      "TEST: [152]\n",
      "TEST: [153]\n",
      "TEST: [154]\n",
      "TEST: [155]\n",
      "TEST: [156]\n",
      "TEST: [157]\n",
      "TEST: [158]\n",
      "TEST: [159]\n",
      "TEST: [160]\n",
      "TEST: [161]\n",
      "TEST: [162]\n",
      "TEST: [163]\n",
      "TEST: [164]\n",
      "TEST: [165]\n",
      "TEST: [166]\n",
      "TEST: [167]\n",
      "TEST: [168]\n",
      "TEST: [169]\n",
      "TEST: [170]\n",
      "TEST: [171]\n",
      "TEST: [172]\n",
      "TEST: [173]\n",
      "TEST: [174]\n",
      "TEST: [175]\n",
      "TEST: [176]\n",
      "TEST: [177]\n",
      "TEST: [178]\n",
      "TEST: [179]\n",
      "TEST: [180]\n",
      "TEST: [181]\n",
      "TEST: [182]\n",
      "TEST: [183]\n",
      "TEST: [184]\n",
      "TEST: [185]\n",
      "TEST: [186]\n",
      "TEST: [187]\n",
      "TEST: [188]\n",
      "TEST: [189]\n",
      "TEST: [190]\n",
      "TEST: [191]\n",
      "TEST: [192]\n",
      "TEST: [193]\n",
      "TEST: [194]\n",
      "TEST: [195]\n",
      "TEST: [196]\n",
      "TEST: [197]\n",
      "TEST: [198]\n",
      "TEST: [199]\n",
      "TEST: [200]\n",
      "TEST: [201]\n",
      "TEST: [202]\n",
      "TEST: [203]\n",
      "TEST: [204]\n",
      "TEST: [205]\n",
      "TEST: [206]\n",
      "TEST: [207]\n",
      "TEST: [208]\n",
      "TEST: [209]\n",
      "TEST: [210]\n",
      "TEST: [211]\n",
      "TEST: [212]\n",
      "TEST: [213]\n",
      "TEST: [214]\n",
      "TEST: [215]\n",
      "TEST: [216]\n",
      "TEST: [217]\n",
      "TEST: [218]\n",
      "TEST: [219]\n",
      "TEST: [220]\n",
      "TEST: [221]\n",
      "TEST: [222]\n",
      "TEST: [223]\n",
      "TEST: [224]\n",
      "TEST: [225]\n",
      "TEST: [226]\n",
      "TEST: [227]\n",
      "TEST: [228]\n",
      "TEST: [229]\n",
      "TEST: [230]\n",
      "TEST: [231]\n",
      "TEST: [232]\n",
      "TEST: [233]\n",
      "TEST: [234]\n",
      "TEST: [235]\n",
      "TEST: [236]\n",
      "TEST: [237]\n",
      "TEST: [238]\n",
      "TEST: [239]\n",
      "TEST: [240]\n",
      "TEST: [241]\n",
      "TEST: [242]\n",
      "TEST: [243]\n",
      "TEST: [244]\n",
      "TEST: [245]\n",
      "TEST: [246]\n",
      "TEST: [247]\n",
      "TEST: [248]\n",
      "TEST: [249]\n",
      "TEST: [250]\n",
      "TEST: [251]\n",
      "TEST: [252]\n",
      "TEST: [253]\n",
      "TEST: [254]\n",
      "TEST: [255]\n",
      "TEST: [256]\n",
      "TEST: [257]\n",
      "TEST: [258]\n",
      "TEST: [259]\n",
      "TEST: [260]\n",
      "TEST: [261]\n",
      "TEST: [262]\n",
      "TEST: [263]\n",
      "TEST: [264]\n",
      "TEST: [265]\n",
      "TEST: [266]\n",
      "TEST: [267]\n",
      "TEST: [268]\n",
      "TEST: [269]\n",
      "TEST: [270]\n",
      "TEST: [271]\n",
      "TEST: [272]\n",
      "TEST: [273]\n",
      "TEST: [274]\n",
      "TEST: [275]\n",
      "TEST: [276]\n",
      "TEST: [277]\n",
      "TEST: [278]\n",
      "TEST: [279]\n",
      "TEST: [280]\n",
      "TEST: [281]\n",
      "TEST: [282]\n",
      "TEST: [283]\n",
      "TEST: [284]\n",
      "TEST: [285]\n",
      "TEST: [286]\n",
      "TEST: [287]\n",
      "TEST: [288]\n",
      "TEST: [289]\n",
      "TEST: [290]\n",
      "TEST: [291]\n",
      "TEST: [292]\n",
      "TEST: [293]\n",
      "TEST: [294]\n",
      "TEST: [295]\n",
      "TEST: [296]\n",
      "TEST: [297]\n",
      "TEST: [298]\n",
      "TEST: [299]\n",
      "TEST: [300]\n",
      "TEST: [301]\n",
      "TEST: [302]\n",
      "TEST: [303]\n",
      "TEST: [304]\n",
      "TEST: [305]\n",
      "TEST: [306]\n",
      "TEST: [307]\n",
      "TEST: [308]\n",
      "TEST: [309]\n",
      "TEST: [310]\n",
      "TEST: [311]\n",
      "TEST: [312]\n",
      "TEST: [313]\n",
      "TEST: [314]\n",
      "TEST: [315]\n",
      "TEST: [316]\n",
      "TEST: [317]\n",
      "TEST: [318]\n",
      "TEST: [319]\n",
      "TEST: [320]\n",
      "TEST: [321]\n",
      "TEST: [322]\n",
      "TEST: [323]\n",
      "TEST: [324]\n",
      "TEST: [325]\n",
      "TEST: [326]\n",
      "TEST: [327]\n",
      "TEST: [328]\n",
      "TEST: [329]\n",
      "TEST: [330]\n",
      "TEST: [331]\n",
      "TEST: [332]\n",
      "TEST: [333]\n",
      "TEST: [334]\n",
      "TEST: [335]\n",
      "TEST: [336]\n",
      "TEST: [337]\n",
      "TEST: [338]\n",
      "TEST: [339]\n",
      "TEST: [340]\n",
      "TEST: [341]\n",
      "TEST: [342]\n",
      "TEST: [343]\n",
      "TEST: [344]\n",
      "TEST: [345]\n",
      "TEST: [346]\n",
      "TEST: [347]\n",
      "TEST: [348]\n",
      "TEST: [349]\n",
      "TEST: [350]\n",
      "TEST: [351]\n",
      "TEST: [352]\n",
      "TEST: [353]\n",
      "TEST: [354]\n",
      "TEST: [355]\n",
      "TEST: [356]\n",
      "TEST: [357]\n",
      "TEST: [358]\n",
      "TEST: [359]\n",
      "TEST: [360]\n",
      "TEST: [361]\n",
      "TEST: [362]\n",
      "TEST: [363]\n",
      "TEST: [364]\n",
      "TEST: [365]\n",
      "TEST: [366]\n",
      "TEST: [367]\n",
      "TEST: [368]\n",
      "TEST: [369]\n",
      "TEST: [370]\n",
      "TEST: [371]\n",
      "TEST: [372]\n",
      "TEST: [373]\n",
      "TEST: [374]\n",
      "TEST: [375]\n",
      "TEST: [376]\n",
      "TEST: [377]\n",
      "TEST: [378]\n",
      "TEST: [379]\n",
      "TEST: [380]\n",
      "TEST: [381]\n",
      "TEST: [382]\n",
      "TEST: [383]\n",
      "TEST: [384]\n",
      "TEST: [385]\n",
      "TEST: [386]\n",
      "TEST: [387]\n",
      "TEST: [388]\n",
      "TEST: [389]\n",
      "TEST: [390]\n",
      "TEST: [391]\n",
      "TEST: [392]\n",
      "TEST: [393]\n",
      "TEST: [394]\n",
      "TEST: [395]\n",
      "TEST: [396]\n",
      "TEST: [397]\n",
      "TEST: [398]\n",
      "TEST: [399]\n",
      "TEST: [400]\n",
      "TEST: [401]\n",
      "TEST: [402]\n",
      "TEST: [403]\n",
      "TEST: [404]\n",
      "TEST: [405]\n",
      "TEST: [406]\n",
      "TEST: [407]\n",
      "TEST: [408]\n",
      "TEST: [409]\n",
      "TEST: [410]\n",
      "TEST: [411]\n",
      "TEST: [412]\n",
      "TEST: [413]\n",
      "TEST: [414]\n",
      "TEST: [415]\n",
      "TEST: [416]\n",
      "TEST: [417]\n",
      "TEST: [418]\n",
      "TEST: [419]\n",
      "TEST: [420]\n",
      "TEST: [421]\n",
      "TEST: [422]\n",
      "TEST: [423]\n",
      "TEST: [424]\n",
      "TEST: [425]\n",
      "TEST: [426]\n",
      "TEST: [427]\n",
      "TEST: [428]\n",
      "TEST: [429]\n",
      "TEST: [430]\n",
      "TEST: [431]\n",
      "TEST: [432]\n",
      "TEST: [433]\n",
      "TEST: [434]\n",
      "TEST: [435]\n",
      "TEST: [436]\n",
      "TEST: [437]\n",
      "TEST: [438]\n",
      "TEST: [439]\n",
      "TEST: [440]\n",
      "TEST: [441]\n",
      "TEST: [442]\n",
      "TEST: [443]\n",
      "TEST: [444]\n",
      "TEST: [445]\n",
      "TEST: [446]\n",
      "TEST: [447]\n",
      "TEST: [448]\n",
      "TEST: [449]\n",
      "TEST: [450]\n",
      "TEST: [451]\n",
      "TEST: [452]\n",
      "TEST: [453]\n",
      "TEST: [454]\n",
      "TEST: [455]\n",
      "TEST: [456]\n",
      "TEST: [457]\n",
      "TEST: [458]\n",
      "TEST: [459]\n",
      "TEST: [460]\n",
      "TEST: [461]\n",
      "TEST: [462]\n",
      "TEST: [463]\n",
      "TEST: [464]\n",
      "TEST: [465]\n",
      "TEST: [466]\n",
      "TEST: [467]\n",
      "TEST: [468]\n",
      "TEST: [469]\n",
      "TEST: [470]\n",
      "TEST: [471]\n",
      "TEST: [472]\n",
      "TEST: [473]\n",
      "TEST: [474]\n",
      "TEST: [475]\n",
      "TEST: [476]\n",
      "TEST: [477]\n",
      "TEST: [478]\n",
      "TEST: [479]\n",
      "TEST: [480]\n",
      "TEST: [481]\n",
      "TEST: [482]\n",
      "TEST: [483]\n",
      "TEST: [484]\n",
      "TEST: [485]\n",
      "TEST: [486]\n",
      "TEST: [487]\n",
      "TEST: [488]\n",
      "TEST: [489]\n",
      "TEST: [490]\n",
      "TEST: [491]\n",
      "TEST: [492]\n",
      "TEST: [493]\n",
      "TEST: [494]\n",
      "TEST: [495]\n",
      "TEST: [496]\n",
      "TEST: [497]\n",
      "TEST: [498]\n",
      "TEST: [499]\n",
      "TEST: [500]\n",
      "TEST: [501]\n",
      "TEST: [502]\n",
      "TEST: [503]\n",
      "TEST: [504]\n",
      "TEST: [505]\n",
      "TEST: [506]\n",
      "TEST: [507]\n",
      "TEST: [508]\n",
      "TEST: [509]\n",
      "TEST: [510]\n",
      "TEST: [511]\n",
      "TEST: [512]\n",
      "TEST: [513]\n",
      "TEST: [514]\n",
      "TEST: [515]\n",
      "TEST: [516]\n",
      "TEST: [517]\n",
      "TEST: [518]\n",
      "TEST: [519]\n",
      "TEST: [520]\n",
      "TEST: [521]\n",
      "TEST: [522]\n",
      "TEST: [523]\n",
      "TEST: [524]\n",
      "TEST: [525]\n",
      "TEST: [526]\n",
      "TEST: [527]\n",
      "TEST: [528]\n",
      "TEST: [529]\n",
      "TEST: [530]\n",
      "TEST: [531]\n",
      "TEST: [532]\n",
      "TEST: [533]\n",
      "TEST: [534]\n",
      "TEST: [535]\n",
      "TEST: [536]\n",
      "TEST: [537]\n",
      "TEST: [538]\n",
      "TEST: [539]\n",
      "TEST: [540]\n",
      "TEST: [541]\n",
      "TEST: [542]\n",
      "TEST: [543]\n",
      "TEST: [544]\n",
      "TEST: [545]\n",
      "TEST: [546]\n",
      "TEST: [547]\n",
      "TEST: [548]\n",
      "TEST: [549]\n",
      "TEST: [550]\n",
      "TEST: [551]\n",
      "TEST: [552]\n",
      "TEST: [553]\n",
      "TEST: [554]\n",
      "TEST: [555]\n",
      "TEST: [556]\n",
      "TEST: [557]\n",
      "TEST: [558]\n",
      "TEST: [559]\n",
      "TEST: [560]\n",
      "TEST: [561]\n",
      "TEST: [562]\n",
      "TEST: [563]\n",
      "TEST: [564]\n",
      "TEST: [565]\n",
      "TEST: [566]\n",
      "TEST: [567]\n",
      "TEST: [568]\n",
      "TEST: [569]\n",
      "TEST: [570]\n",
      "TEST: [571]\n",
      "TEST: [572]\n",
      "TEST: [573]\n",
      "TEST: [574]\n",
      "TEST: [575]\n",
      "TEST: [576]\n",
      "TEST: [577]\n",
      "TEST: [578]\n",
      "TEST: [579]\n",
      "TEST: [580]\n",
      "TEST: [581]\n",
      "TEST: [582]\n",
      "TEST: [583]\n",
      "TEST: [584]\n",
      "TEST: [585]\n",
      "TEST: [586]\n",
      "TEST: [587]\n",
      "TEST: [588]\n",
      "TEST: [589]\n",
      "TEST: [590]\n",
      "TEST: [591]\n",
      "TEST: [592]\n",
      "TEST: [593]\n",
      "TEST: [594]\n",
      "TEST: [595]\n",
      "TEST: [596]\n",
      "TEST: [597]\n",
      "TEST: [598]\n",
      "TEST: [599]\n",
      "TEST: [600]\n",
      "TEST: [601]\n",
      "TEST: [602]\n",
      "TEST: [603]\n",
      "TEST: [604]\n",
      "TEST: [605]\n",
      "TEST: [606]\n",
      "TEST: [607]\n",
      "TEST: [608]\n",
      "TEST: [609]\n",
      "TEST: [610]\n",
      "TEST: [611]\n",
      "TEST: [612]\n",
      "TEST: [613]\n",
      "TEST: [614]\n",
      "TEST: [615]\n",
      "TEST: [616]\n",
      "TEST: [617]\n",
      "TEST: [618]\n",
      "TEST: [619]\n",
      "TEST: [620]\n",
      "TEST: [621]\n",
      "TEST: [622]\n",
      "TEST: [623]\n",
      "TEST: [624]\n",
      "TEST: [625]\n",
      "TEST: [626]\n",
      "TEST: [627]\n",
      "TEST: [628]\n",
      "TEST: [629]\n",
      "TEST: [630]\n",
      "TEST: [631]\n",
      "TEST: [632]\n",
      "TEST: [633]\n",
      "TEST: [634]\n",
      "TEST: [635]\n",
      "TEST: [636]\n",
      "TEST: [637]\n",
      "TEST: [638]\n",
      "TEST: [639]\n",
      "TEST: [640]\n",
      "0.7956318252730109 0.40323916462286735\n",
      "0.9017160686427457 0.2976981696185195\n",
      "0.9391575663026521 0.2390410675158805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "# combine model score\n",
    "countvector_cols = ['lemma', 'keyword', 'noun', 'verb']\n",
    "top_clusters_cols = ['clusters_1', 'clusters_2', 'clusters_3']\n",
    "\n",
    "feature_cols = countvector_cols + top_clusters_cols\n",
    "\n",
    "# LeaveOneOut coss validation \n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(data[feature_cols], data['target'])\n",
    "\n",
    "print(loo)\n",
    "\n",
    "cv_scores_top1 = []\n",
    "cv_scores_top2 = []\n",
    "cv_scores_top3 = []\n",
    "\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in loo.split(data[feature_cols], data['target']):\n",
    "    print(\"TEST:\", test_index)\n",
    "    # get train, test data for each chunk \n",
    "    X_train, X_test = data.loc[train_index,feature_cols], data.loc[test_index,feature_cols]\n",
    "    y_train, y_test = data.loc[train_index,'target'], data.loc[test_index,'target']\n",
    "    \n",
    "    v_lemma = TfidfVectorizer()\n",
    "    x_train_lemma = v_lemma.fit_transform(X_train['lemma'])\n",
    "    x_test_lemma = v_lemma.transform(X_test['lemma'])\n",
    "    vocab_lemma = dict(v_lemma.vocabulary_)\n",
    "    \n",
    "    v_keyword = TfidfVectorizer()\n",
    "    x_train_keyword = v_keyword.fit_transform(X_train['keyword'])\n",
    "    x_test_keyword = v_keyword.transform(X_test['keyword'])\n",
    "    vocab_keyword = dict(v_keyword.vocabulary_)\n",
    "    \n",
    "    v_noun = TfidfVectorizer()\n",
    "    x_train_noun = v_noun.fit_transform(X_train['noun'])\n",
    "    x_test_noun = v_noun.transform(X_test['noun'])\n",
    "    vocab_noun = dict(v_noun.vocabulary_)\n",
    "    \n",
    "    v_verb = TfidfVectorizer()\n",
    "    x_train_verb = v_verb.fit_transform(X_train['verb'])\n",
    "    x_test_verb = v_verb.transform(X_test['verb'])\n",
    "    vocab_verb = dict(v_verb.vocabulary_)\n",
    "\n",
    "    # combine all features \n",
    "    x_train_combined = hstack((x_train_lemma,x_train_keyword,x_train_noun,x_train_verb,X_train[top_clusters_cols].values),format='csr')\n",
    "    x_train_combined_columns= v_lemma.get_feature_names()+v_keyword.get_feature_names()+v_noun.get_feature_names()+v_verb.get_feature_names()+top_clusters_cols\n",
    "    \n",
    "    x_test_combined = hstack((x_test_lemma,x_test_keyword,x_test_noun,x_test_verb,X_test[top_clusters_cols].values),format='csr')\n",
    "    x_test_combined_columns= v_lemma.get_feature_names()+v_keyword.get_feature_names()+v_noun.get_feature_names()+v_verb.get_feature_names()+top_clusters_cols\n",
    "   \n",
    "    x_train_combined = pd.DataFrame(x_train_combined.toarray())\n",
    "    x_train_combined.columns = x_train_combined_columns\n",
    "    \n",
    "    x_test_combined = pd.DataFrame(x_test_combined.toarray())\n",
    "    x_test_combined.columns = x_test_combined_columns\n",
    "    \n",
    "    # build classifier\n",
    "    clf = RandomForestClassifier(max_depth=50, n_estimators=1000)\n",
    "    clf.fit(x_train_combined, y_train)\n",
    "    \n",
    "    \n",
    "    probs = clf.predict_proba(x_test_combined)\n",
    "    best_3 = pd.DataFrame(np.argsort(probs, axis=1)[:,-3:],columns=['top3','top2','top1'])\n",
    "    best_3['top1'] = clf.classes_[best_3['top1']]\n",
    "    best_3['top2'] = clf.classes_[best_3['top2']]\n",
    "    best_3['top3'] = clf.classes_[best_3['top3']]\n",
    "    \n",
    "    result = pd.concat([best_3.reset_index(drop=True),pd.DataFrame(y_test).reset_index(drop=True), X_test[feature_cols].reset_index(drop=True)], axis=1)\n",
    "    final_result = pd.concat([final_result, result])\n",
    "    \n",
    "    score_1 = result[result['top1'] == result['target']].shape[0] / result.shape[0]\n",
    "    score_2 = result[(result['top1'] == result['target']) | (result['top2'] == result['target'])].shape[0] / result.shape[0]\n",
    "    score_3 = result[(result['top1'] == result['target']) | (result['top2'] == result['target'])| (result['top3'] == result['target'])].shape[0] / result.shape[0]\n",
    "    cv_scores_top1.append(score_1)\n",
    "    cv_scores_top2.append(score_2)\n",
    "    cv_scores_top3.append(score_3)\n",
    "    \n",
    "print(np.mean(np.array((cv_scores_top1))), np.std(np.array((cv_scores_top1))))\n",
    "print(np.mean(np.array((cv_scores_top2))), np.std(np.array((cv_scores_top2))))\n",
    "print(np.mean(np.array((cv_scores_top3))), np.std(np.array((cv_scores_top3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model -  tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T07:37:50.347320Z",
     "start_time": "2020-04-01T07:37:47.878482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/alert/lib/python3.6/site-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
      "\n",
      "The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7722141472868217 0.01956516439124754\n",
      "0.8626937984496124 0.02307247153203572\n",
      "0.9157340116279069 0.016775988754018624\n"
     ]
    }
   ],
   "source": [
    "# combine model score\n",
    "feature_cols = ['Questions']\n",
    "\n",
    "# StratifiedKFold coss validation \n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(data[feature_cols], data['target'])\n",
    "print(skf)\n",
    "cv_scores = []\n",
    "cv_scores_top1 = []\n",
    "cv_scores_top2 = []\n",
    "cv_scores_top3 = []\n",
    "\n",
    "for train_index, test_index in skf.split(data[feature_cols], data['target']):\n",
    "    # get train, test data for each chunk \n",
    "    X_train, X_test = data.loc[train_index,feature_cols], data.loc[test_index,feature_cols]\n",
    "    y_train, y_test = data.loc[train_index,'target'], data.loc[test_index,'target']\n",
    "    \n",
    "    v = TfidfVectorizer(ngram_range=(1, 1))\n",
    "    x_train = v.fit_transform(X_train['Questions'])\n",
    "    x_test = v.transform(X_test['Questions'])\n",
    "    \n",
    "    x_train = pd.DataFrame(x_train.toarray())\n",
    "    x_test = pd.DataFrame(x_test.toarray())\n",
    "    \n",
    "    x_train.columns = v.get_feature_names()\n",
    "    x_test.columns = v.get_feature_names()\n",
    "    \n",
    "    # build classifier\n",
    "    clf = RandomForestClassifier(max_depth=25, n_estimators=300)\n",
    "    clf.fit(x_train, y_train)\n",
    "    score = clf.score(x_test, y_test)\n",
    "    cv_scores.append(score)\n",
    "    \n",
    "    probs = clf.predict_proba(x_test)\n",
    "    best_3 = pd.DataFrame(np.argsort(probs, axis=1)[:,-3:],columns=['top3','top2','top1'])\n",
    "    best_3['top1'] = clf.classes_[best_3['top1']]\n",
    "    best_3['top2'] = clf.classes_[best_3['top2']]\n",
    "    best_3['top3'] = clf.classes_[best_3['top3']]\n",
    "    result = pd.concat([best_3.reset_index(drop=True),pd.DataFrame(y_test).reset_index(drop=True)], axis=1)\n",
    "    score_1 = result[result['top1'] == result['target']].shape[0] / result.shape[0]\n",
    "    score_2 = result[(result['top1'] == result['target']) | (result['top2'] == result['target'])].shape[0] / result.shape[0]\n",
    "    score_3 = result[(result['top1'] == result['target']) | (result['top2'] == result['target'])| (result['top3'] == result['target'])].shape[0] / result.shape[0]\n",
    "    cv_scores_top1.append(score_1)\n",
    "    cv_scores_top2.append(score_2)\n",
    "    cv_scores_top3.append(score_3)\n",
    "    \n",
    "    \n",
    "print(np.mean(np.array((cv_scores_top1))), np.std(np.array((cv_scores_top1))))\n",
    "print(np.mean(np.array((cv_scores_top2))), np.std(np.array((cv_scores_top2))))\n",
    "print(np.mean(np.array((cv_scores_top3))), np.std(np.array((cv_scores_top3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alert]",
   "language": "python",
   "name": "conda-env-alert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222.35px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
