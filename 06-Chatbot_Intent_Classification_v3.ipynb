{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The comma separated dataset has been minimally preprocessed with dropna.\n",
    "\n",
    "It has only 2 columns:\n",
    "* intent\n",
    "    * the target for the classifier to predict\n",
    "* query\n",
    "    * the input of user question to the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Import\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PATH_HOME = Path.home()\n",
    "PATH_PROJ = Path.cwd()\n",
    "PATH_DATA = PATH_PROJ\n",
    "\n",
    "sys.path.append(str(PATH_PROJ))\n",
    "\n",
    "def get_data(path=PATH_DATA/'data.csv'):\n",
    "    \"\"\" load data from csv \"\"\"\n",
    "    df = pd.read_csv(path, usecols=['intent', 'query'])\n",
    "    df = df.dropna().drop_duplicates()\n",
    "    # From EDA, the intent only have 1 example\n",
    "    df = df.drop(df[df.intent == 'Late fee waiver for credit card'].index)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "df = get_data()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>what promotions do you have?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>what promotions are available?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>promotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>I want to see promotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>view promotions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       intent                           query\n",
       "0  Promotions    what promotions do you have?\n",
       "1  Promotions  what promotions are available?\n",
       "2  Promotions                      promotions\n",
       "3  Promotions        I want to see promotions\n",
       "4  Promotions                 view promotions"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['intent'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "This section store all the utility **functions** and **classes** created by XX, WJ, Nan to faciliate the text intent classification process.\n",
    "\n",
    "* Some specific libraries are loaded in the same cell as function\n",
    "* Cells marked with **TEST** are for testing purpose only\n",
    "* Docstring style guide \n",
    "    * https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\n",
    "    * http://google.github.io/styleguide/pyguide.html#384-classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    \"\"\" Basic text cleaning\n",
    "        \n",
    "        1. lowercase\n",
    "        2. remove special characters\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def nltk_tokenize(text):\n",
    "    \"\"\" tokenize text using NLTK and join back as sentence\"\"\"\n",
    "    # import nltk\n",
    "    # nltk.download('punkt')\n",
    "    return ' '.join(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for spacy tokenizer\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n",
      "['run', 'run', 'run', 'runner']\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "print(spacy_tokenizer(\"hello World!\"))\n",
    "print(spacy_tokenizer(\"run runs running runner\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for spacy pipeline including tokenizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "class SpacyPipeline():\n",
    "    \"\"\" Utilize Spacy Pipeline \"\"\"\n",
    "    def __init__(self, word_vector='en_core_web_lg'):\n",
    "        self.nlp = spacy(word_vector)\n",
    "\n",
    "    def tokenize(self, text, lemma=True, stop_words=True):\n",
    "        \"\"\" Return tokenized text \n",
    "            \"hello world!\" -> [\"hello\", \"world\"]\n",
    "        \"\"\"\n",
    "        self.tokens = self.nlp(text)\n",
    "        \n",
    "        if lemma:\n",
    "            self.tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in self.tokens]\n",
    "        \n",
    "        if stop_words:\n",
    "            self.tokens = [word for word in self.tokens if word not in STOP_WORDS and word not in string.punctuations]\n",
    "\n",
    "        return self.tokens\n",
    "\n",
    "    def get_word_embed(self, text):\n",
    "        \"\"\" Get individual word embedding: result.shape = (num_of_words, 300)\"\"\"\n",
    "        with self.nlp.disable_pipes():\n",
    "            vectors = np.array([token.vector for token in self.nlp(text)])\n",
    "        return vectors\n",
    "\n",
    "    def get_doc_embed(self, doc):\n",
    "        \"\"\" Get sentence embeddings based on average of word embeddings of each sentence\n",
    "            \n",
    "            Result.shape = (num_of_sentences, 300)\n",
    "            \n",
    "            Args:\n",
    "                doc (pd.Series): series of sentences\n",
    "\n",
    "            Returns:\n",
    "                doc_vectors (np.array): embedding matrix of document with each row is a embedding of that sentence\n",
    "        \"\"\"\n",
    "        with self.nlp.disable_pipes():\n",
    "            doc_vectors = np.array([self.nlp(text).vector for text in doc])\n",
    "        return doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(sentences):\n",
    "    \"\"\" Get idf dictionary\n",
    "    \n",
    "    Args:\n",
    "        sentences (list): list of input sentences (str)\n",
    "\n",
    "    Returns:\n",
    "        idf (dict): idf[word] = inverse document frequency of that word in all training queries\n",
    "    \"\"\"\n",
    "    num_of_sentences = len(sentences)\n",
    "    doc_freq = {}  # data frequency    \n",
    "    for sentence in sentences:\n",
    "        # words = set(sentence.strip().split())\n",
    "        words = spacy_tokenizer(sentence)\n",
    "        for word in words:\n",
    "            if word not in doc_freq:\n",
    "                doc_freq[word] = 0.0\n",
    "            doc_freq[word] += 1.0\n",
    "    print(doc_freq)\n",
    "    # to smooth and avoid zero divide, add 1 to count\n",
    "    for word, count in doc_freq.items():\n",
    "        print(float(num_of_sentences)/(count))\n",
    "    idf = {word:math.log(float(num_of_sentences)/(count)) for word, count in doc_freq.items() }\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quick': 1.0, 'brown': 1.0, 'fox': 2.0, 'jump': 1.0, 'lazy': 1.0, 'dog': 2.0}\n",
      "3.0\n",
      "3.0\n",
      "1.5\n",
      "3.0\n",
      "3.0\n",
      "1.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'quick': 1.0986122886681098,\n",
       " 'brown': 1.0986122886681098,\n",
       " 'fox': 0.4054651081081644,\n",
       " 'jump': 1.0986122886681098,\n",
       " 'lazy': 1.0986122886681098,\n",
       " 'dog': 0.4054651081081644}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The dog.\",\n",
    "        \"The fox\"]\n",
    "test_idf = get_idf(text)\n",
    "test_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf_TfidfVectorizer(sentences):\n",
    "    \"\"\" Get idf dictionary by using TfidfVectorizer\n",
    "    \n",
    "    Args:\n",
    "        sentences (list): list of input sentences (str)\n",
    "\n",
    "    Returns:\n",
    "        idf (dict): idf[word] = inverse document frequency of that word in all training queries\n",
    "    \"\"\"\n",
    "    # use customized Spacy tokenizer\n",
    "    vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
    "    vectorizer.fit(sentences)\n",
    "    # TODO: normalize the idf weights\n",
    "    idf = {k:vectorizer.idf_[v] for k,v in vectorizer.vocabulary_.items()}\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/alert/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'quick': 1.6931471805599454,\n",
       " 'brown': 1.6931471805599454,\n",
       " 'fox': 1.2876820724517808,\n",
       " 'jump': 1.6931471805599454,\n",
       " 'lazy': 1.6931471805599454,\n",
       " 'dog': 1.2876820724517808}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The dog.\",\n",
    "        \"The fox\"]\n",
    "test_idf = get_idf_TfidfVectorizer(text)\n",
    "test_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same tokenizer as WJ\n",
    "def tokenize(wd): \n",
    "    return ' '.join(word_tokenize(wd))\n",
    "df['query'] = df['query'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vec(sentence, word2vec, idf=None):\n",
    "    \"\"\" Get embedding of sentence by using word2vec embedding of words\n",
    "    \n",
    "    If idf is provided, the sentence is the weighted embedding by\n",
    "        SUM( embedding[word] x idf[word] )\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): input sentence\n",
    "        word2vec (dict): loaded word2vec model from Gensim\n",
    "        idf (dict, optional): inverse document frequency of words in all queries\n",
    "\n",
    "    Returns:\n",
    "        emb (np.array): 300-dimentions embedding of sentence\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    words = [word for word in words if word in word2vec.vocab]\n",
    "    \n",
    "    # if no word in word2vec vocab, return 0x300 embedding\n",
    "    if len(words)==0:\n",
    "        return np.zeros((300,), dtype='float32')\n",
    "    \n",
    "    # use mean if no idf provided\n",
    "    if idf is None:\n",
    "        emb = word2vec[words].mean(axis=0)\n",
    "    else:\n",
    "        # get all idf of words, if new word is not in idf, assign 0.0 weights\n",
    "        idf_series = np.array([idf.get(word, 0.0) for word in words])\n",
    "        # change shape to 1 x num_of_words\n",
    "        idf_series = idf_series.reshape(1, -1)\n",
    "        # use matrix multiplication to get weighted word vector sum for sentence embeddings\n",
    "        emb = np.matmul(idf_series, word2vec[words]).reshape(-1)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf shape \t(1 x number of words)\t\t -  (1, 3)\n",
      "word2vec shape \t(number of words x 300)\t\t -  (3, 300)\n",
      "result shape \t(1-dimentional: np.array)\t -  (300,)\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "import gensim.downloader as api\n",
    "\n",
    "def test_word2vec_idf(word2vec):\n",
    "    \"\"\" Test the algorithm for get_sentence_vec is giving the right output \"\"\"\n",
    "    # setup\n",
    "    sen = \"test cat and dog\"\n",
    "    idf = {\"test\":1/3, \"cat\":1/3, \"and\":0, \"dog\":1/3}\n",
    "    \n",
    "    words = sen.split()\n",
    "    words = [word for word in words if word in word2vec.vocab]\n",
    "    \n",
    "    idf_series = np.array([idf[word] for word in words])\n",
    "    print(\"idf shape \\t(1 x number of words)\\t\\t - \", idf_series.reshape(1, -1).shape)\n",
    "    idf_series = idf_series.reshape(1, -1)\n",
    "    \n",
    "    print(\"word2vec shape \\t(number of words x 300)\\t\\t - \", word2vec[words].shape)\n",
    "    result = np.matmul(idf_series.reshape(1, -1), word2vec[words])\n",
    "    result = result.reshape(-1)\n",
    "    print(\"result shape \\t(1-dimentional: np.array)\\t - \", result.shape)\n",
    "    \n",
    "    emb1 = word2vec[words].mean(axis=0)\n",
    "    emb2 = result\n",
    "    \n",
    "    assert emb1.all() == emb2.all()\n",
    "    # return result\n",
    "\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load(\"word2vec-google-news-300\")  \n",
    "\n",
    "test_word2vec_idf(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_centre(sentences, word2vec, idf=None, num_features=300):\n",
    "    \"\"\" Get sentences centre by averaging all embeddings of sentences in a list\n",
    "    \n",
    "    Depends on function get_sentence_vec()\n",
    "    \n",
    "    Args:\n",
    "        sentence (list): list of input sentences (str)\n",
    "        word2vec (dict): loaded word2vec model from Gensim\n",
    "        idf (dict, optional): inverse document frequency of words in all queries\n",
    "\n",
    "    Returns:\n",
    "        emb (np.array): 300-dimentions embedding of sentence\n",
    "    \"\"\"\n",
    "    # convert list of sentences to their vectors\n",
    "    sentences_vec = [get_sentence_vec(sentence, word2vec, idf) for sentence in sentences]\n",
    "    \n",
    "    # each row in matrix is 300 dimensions embedding of a sentence\n",
    "    sentences_matrix = np.vstack(sentences_vec)\n",
    "    # print(sentences_matrix.shape)\n",
    "    \n",
    "    # average of all rows, take mean at y-axis\n",
    "    sentences_centre = sentences_matrix.mean(axis=0)\n",
    "    \n",
    "    # result should be (300,) same as single sentence\n",
    "    # print(sentences_centre.shape)\n",
    "    return sentences_centre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load(\"word2vec-google-news-300\")  \n",
    "\n",
    "test_sentence_list = [\"hello world\", \"test cat and dog\"]\n",
    "get_sentences_centre(test_sentence_list, word2vec).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get intent list from dataframe[\"intent\"]\n",
    "# intent_list = df.intent.unique().tolist()\n",
    "def get_cluster_centre(df, intent_list, word2vec, idf=None):\n",
    "    \"\"\" get intent cluster centre based on intent list and word embeddings\n",
    "    \n",
    "    Depends on function get_sentences_centre()\n",
    "    \n",
    "    Args:\n",
    "        intent_list (list): List of unique intents(str)\n",
    "        word2vec (dict): word embeddings dictionary \n",
    "\n",
    "    Returns:\n",
    "        result (dict): intent cluster centres in dictionary format - {intent1:embedding1, intent2:embedding2,...}\n",
    "    \"\"\" \n",
    "    result = {intent:get_sentences_centre(df[df.intent == intent]['query'].values, word2vec, idf) for intent in intent_list}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "data = {'intent':  ['hello', 'cat'],\n",
    "        'query': ['hello world', 'test cat and dog']\n",
    "        }\n",
    "\n",
    "test_df = pd.DataFrame (data, columns = ['intent','query'])\n",
    "test_intents = ['hello', 'cat']\n",
    "\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load(\"word2vec-google-news-300\") \n",
    "\n",
    "result = get_cluster_centre(test_df, test_intents, word2vec)\n",
    "result[test_intents[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(df_in, word2vec, leave_one_out=False, idf_flag=False):\n",
    "    \"\"\" Get distance for each query to every intent center\n",
    "    \n",
    "    Depends on function get_cluster_centre()\n",
    "    \n",
    "    Args:\n",
    "        df_in (pd.DataFrame): input dataframe with intent and query\n",
    "        word2vec (dict): word embeddings dictionary \n",
    "        leave_one_out (bool): whether leave the input query out of training\n",
    "        idf (bool): whether use weighted word vectors to get sentence embedding\n",
    "\n",
    "    Returns:\n",
    "        result (pd.DataFrame): distance matrix for each query, lowest distance intent idealy should match label\n",
    "    \"\"\"\n",
    "    df = df_in.copy()\n",
    "    intent_list = df.intent.unique().tolist()\n",
    "    \n",
    "    if leave_one_out:\n",
    "        # print(\"Leave one out\")\n",
    "        sentence_distance = []\n",
    "        \n",
    "        for ind in df.index:\n",
    "            sentence_distance_tmp = []\n",
    "            query = df.loc[ind, 'query']\n",
    "            df_data = df.drop(ind)\n",
    "            \n",
    "            if idf_flag:\n",
    "                idf = get_idf_TfidfVectorizer(df['query'].tolist())\n",
    "            else:\n",
    "                # print(\"No IDF\")\n",
    "                idf = None\n",
    "            \n",
    "            sentence_centre_dic = get_cluster_centre(df_data, intent_list, word2vec, idf)\n",
    "            for intent in intent_list:\n",
    "                sentence_distance_tmp.append(cosine_distances(get_sentence_vec(query, word2vec, idf).reshape(1,-1), \n",
    "                                                              sentence_centre_dic[intent].reshape(1,-1)).item())\n",
    "            sentence_distance.append(sentence_distance_tmp)\n",
    "\n",
    "        df_sentence_distance = pd.DataFrame(sentence_distance, columns=intent_list)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        result = pd.concat([df, df_sentence_distance], axis=1)\n",
    "    \n",
    "    else:\n",
    "        sentence_centre_dic = get_cluster_centre(df, intent_list, word2vec)\n",
    "        # build dataframe that contains distance between each query to all intent cluster centre\n",
    "        for intent in intent_list:\n",
    "            # distance = cosine_similarity(sentence embedding, intent cluster centre embedding)\n",
    "            df[intent] = df['query'].apply(lambda x: cosine_distances(get_sentence_vec(x, word2vec).reshape(1,-1), \n",
    "                                                                      sentence_centre_dic[intent].reshape(1,-1)).item())\n",
    "        result = df\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_distance_matrix(df_in):\n",
    "    \"\"\" Evaluate distance matrix by compare closest intent center and label \"\"\"\n",
    "    df = df_in.copy()\n",
    "    df.set_index(['intent', 'query'], inplace=True)\n",
    "    df['cluster'] = df.idxmin(axis=1)\n",
    "    df.reset_index(inplace=True)\n",
    "    df['correct'] = (df.cluster == df.intent)\n",
    "    result = sum(df.correct) / len(df)\n",
    "    # print(\"Accuracy for distance-based classification is\", '{:.2%}'.format(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "def test_clustering_accuracy(df_in, word2vec):\n",
    "    \"\"\" test accuracy based on distance of sentence to each cluster center\"\"\"\n",
    "    df_result = get_distance_matrix(df_in, word2vec)\n",
    "    # print(df_result.head())\n",
    "    accuracy = evaluate_distance_matrix(df_result)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for text embedding distance is 90.36%\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load(\"word2vec-google-news-300\")  \n",
    "\n",
    "test_result = test_clustering_accuracy(df, word2vec)\n",
    "print(\"Accuracy for text embedding distance is\", '{:.2%}'.format(test_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "def test_leave_one_out_acc(df_in, word2vec):\n",
    "    df_result = get_distance_matrix(df_in, word2vec, leave_one_out=True)\n",
    "    # print(df_result.head())\n",
    "    accuracy = evaluate_distance_matrix(df_result)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for leave one out is 77.66%\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load(\"word2vec-google-news-300\")  \n",
    "\n",
    "test_result = test_leave_one_out_acc(df, word2vec)\n",
    "print(\"Accuracy for leave one out is\", '{:.2%}'.format(test_result)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "def test_idf_acc(df_in, word2vec):\n",
    "    df_result = get_distance_matrix(df_in, word2vec, leave_one_out=True, idf_flag=True)\n",
    "    # print(df_result.head())\n",
    "    accuracy = evaluate_distance_matrix(df_result)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further preprocessing\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stop_words = list(STOP_WORDS)\n",
    "\n",
    "df['query'] = df['query'].apply(lambda x:' '.join([token.lemma_ for token in nlp(x) if token.lemma_ not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for leave one out & use IDF is 86.04%\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load(\"word2vec-google-news-300\")  \n",
    "\n",
    "test_result = test_idf_acc(df, word2vec)\n",
    "print(\"Accuracy for leave one out & use IDF is\", '{:.2%}'.format(test_result)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alert]",
   "language": "python",
   "name": "conda-env-alert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
