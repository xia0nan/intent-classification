{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The comma separated dataset has been minimally preprocessed with dropna.\n",
    "\n",
    "It has only 2 columns:\n",
    "* intent\n",
    "    * the target for the classifier to predict\n",
    "* query\n",
    "    * the input of user question to the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Import\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PATH_HOME = Path.home()\n",
    "PATH_PROJ = Path.cwd()\n",
    "PATH_DATA = PATH_PROJ\n",
    "\n",
    "sys.path.append(str(PATH_PROJ))\n",
    "\n",
    "def get_data(path=PATH_DATA/'data.csv'):\n",
    "    \"\"\" load data from csv \"\"\"\n",
    "    df = pd.read_csv(path, usecols=['intent', 'query'])\n",
    "    df = df.dropna().drop_duplicates()\n",
    "    # From EDA, the intent only have 1 example\n",
    "    df = df.drop(df[df.intent == 'Late fee waiver for credit card'].index)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "df = get_data()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>what promotions do you have?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>what promotions are available?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>promotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>I want to see promotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>view promotions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       intent                           query\n",
       "0  Promotions    what promotions do you have?\n",
       "1  Promotions  what promotions are available?\n",
       "2  Promotions                      promotions\n",
       "3  Promotions        I want to see promotions\n",
       "4  Promotions                 view promotions"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['intent'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "This section store all the utility **functions** and **classes** created by XX, WJ, Nan to faciliate the text intent classification process.\n",
    "\n",
    "* Some specific libraries are loaded in the same cell as function\n",
    "* Cells marked with **TEST** are for testing purpose only\n",
    "* Docstring style guide \n",
    "    * https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\n",
    "    * http://google.github.io/styleguide/pyguide.html#384-classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    \"\"\" Basic text cleaning\n",
    "        \n",
    "        1. lowercase\n",
    "        2. remove special characters\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def nltk_tokenize(text):\n",
    "    \"\"\" tokenize text using NLTK and join back as sentence\"\"\"\n",
    "    # import nltk\n",
    "    # nltk.download('punkt')\n",
    "    return ' '.join(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for spacy tokenizer\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n",
      "['run', 'run', 'run', 'runner']\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "print(spacy_tokenizer(\"hello World!\"))\n",
    "print(spacy_tokenizer(\"run runs running runner\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for spacy pipeline including tokenizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "class SpacyPipeline():\n",
    "    \"\"\" Utilize Spacy Pipeline \"\"\"\n",
    "    def __init__(self, word_vector='en_core_web_lg'):\n",
    "        self.nlp = spacy(word_vector)\n",
    "\n",
    "    def tokenize(self, text, lemma=True, stop_words=True):\n",
    "        \"\"\" Return tokenized text \n",
    "            \"hello world!\" -> [\"hello\", \"world\"]\n",
    "        \"\"\"\n",
    "        self.tokens = self.nlp(text)\n",
    "        \n",
    "        if lemma:\n",
    "            self.tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in self.tokens]\n",
    "        \n",
    "        if stop_words:\n",
    "            self.tokens = [word for word in self.tokens if word not in STOP_WORDS and word not in string.punctuations]\n",
    "\n",
    "        return self.tokens\n",
    "\n",
    "    def get_word_embed(self, text):\n",
    "        \"\"\" Get individual word embedding: result.shape = (num_of_words, 300)\"\"\"\n",
    "        with self.nlp.disable_pipes():\n",
    "            vectors = np.array([token.vector for token in self.nlp(text)])\n",
    "        return vectors\n",
    "\n",
    "    def get_doc_embed(self, doc):\n",
    "        \"\"\" Get sentence embeddings based on average of word embeddings of each sentence\n",
    "            \n",
    "            Result.shape = (num_of_sentences, 300)\n",
    "            \n",
    "            Args:\n",
    "                doc (pd.Series): series of sentences\n",
    "\n",
    "            Returns:\n",
    "                doc_vectors (np.array): embedding matrix of document with each row is a embedding of that sentence\n",
    "        \"\"\"\n",
    "        with self.nlp.disable_pipes():\n",
    "            doc_vectors = np.array([self.nlp(text).vector for text in doc])\n",
    "        return doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(sentences):\n",
    "    \"\"\" Get idf dictionary\n",
    "    \n",
    "    Args:\n",
    "        sentences (list): list of input sentences (str)\n",
    "\n",
    "    Returns:\n",
    "        idf (dict): idf[word] = inverse document frequency of that word in all training queries\n",
    "    \"\"\"\n",
    "    num_of_sentences = len(sentences)\n",
    "    doc_freq = {}  # data frequency    \n",
    "    for sentence in sentences:\n",
    "        # words = set(sentence.strip().split())\n",
    "        words = spacy_tokenizer(sentence)\n",
    "        for word in words:\n",
    "            if word not in doc_freq:\n",
    "                doc_freq[word] = 0.0\n",
    "            doc_freq[word] += 1.0\n",
    "    print(doc_freq)\n",
    "    # to smooth and avoid zero divide, add 1 to count\n",
    "    for word, count in doc_freq.items():\n",
    "        print(float(num_of_sentences)/(count))\n",
    "    idf = {word:math.log(float(num_of_sentences)/(count)) for word, count in doc_freq.items() }\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quick': 1.0, 'brown': 1.0, 'fox': 2.0, 'jump': 1.0, 'lazy': 1.0, 'dog': 2.0}\n",
      "3.0\n",
      "3.0\n",
      "1.5\n",
      "3.0\n",
      "3.0\n",
      "1.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'quick': 1.0986122886681098,\n",
       " 'brown': 1.0986122886681098,\n",
       " 'fox': 0.4054651081081644,\n",
       " 'jump': 1.0986122886681098,\n",
       " 'lazy': 1.0986122886681098,\n",
       " 'dog': 0.4054651081081644}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The dog.\",\n",
    "        \"The fox\"]\n",
    "test_idf = get_idf(text)\n",
    "test_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf_TfidfVectorizer(sentences):\n",
    "    \"\"\" Get idf dictionary by using TfidfVectorizer\n",
    "    \n",
    "    Args:\n",
    "        sentences (list): list of input sentences (str)\n",
    "\n",
    "    Returns:\n",
    "        idf (dict): idf[word] = inverse document frequency of that word in all training queries\n",
    "    \"\"\"\n",
    "    # use customized Spacy tokenizer\n",
    "    vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
    "    vectorizer.fit(sentences)\n",
    "    # TODO: normalize the idf weights\n",
    "    idf = {k:vectorizer.idf_[v] for k,v in vectorizer.vocabulary_.items()}\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/alert/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'quick': 1.6931471805599454,\n",
       " 'brown': 1.6931471805599454,\n",
       " 'fox': 1.2876820724517808,\n",
       " 'jump': 1.6931471805599454,\n",
       " 'lazy': 1.6931471805599454,\n",
       " 'dog': 1.2876820724517808}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The dog.\",\n",
    "        \"The fox\"]\n",
    "test_idf = get_idf_TfidfVectorizer(text)\n",
    "test_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same tokenizer as WJ\n",
    "def tokenize(wd): \n",
    "    return ' '.join(word_tokenize(wd))\n",
    "df['query'] = df['query'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vec(sentence, word2vec, idf=None):\n",
    "    \"\"\" Get embedding of sentence by using word2vec embedding of words\n",
    "    \n",
    "    If idf is provided, the sentence is the weighted embedding by\n",
    "        SUM( embedding[word] x idf[word] )\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): input sentence\n",
    "        word2vec (dict): loaded word2vec model from Gensim\n",
    "        idf (dict, optional): inverse document frequency of words in all queries\n",
    "\n",
    "    Returns:\n",
    "        emb (np.array): 300-dimentions embedding of sentence\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    words = [word for word in words if word in word2vec.vocab]\n",
    "    \n",
    "    # if no word in word2vec vocab, return 0x300 embedding\n",
    "    if len(words)==0:\n",
    "        return np.zeros((300,), dtype='float32')\n",
    "    \n",
    "    # use mean if no idf provided\n",
    "    if idf is None:\n",
    "        emb = word2vec[words].mean(axis=0)\n",
    "    else:\n",
    "        # get all idf of words, if new word is not in idf, assign 0.0 weights\n",
    "        idf_series = np.array([idf.get(word, 0.0) for word in words])\n",
    "        # change shape to 1 x num_of_words\n",
    "        idf_series = idf_series.reshape(1, -1)\n",
    "        # use matrix multiplication to get weighted word vector sum for sentence embeddings\n",
    "        emb = np.matmul(idf_series, word2vec[words]).reshape(-1)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf shape \t(1 x number of words)\t\t -  (1, 3)\n",
      "word2vec shape \t(number of words x 300)\t\t -  (3, 300)\n",
      "result shape \t(1-dimentional: np.array)\t -  (300,)\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "import gensim.downloader as api\n",
    "\n",
    "def test_word2vec_idf(word2vec):\n",
    "    \"\"\" Test the algorithm for get_sentence_vec is giving the right output \"\"\"\n",
    "    # setup\n",
    "    sen = \"test cat and dog\"\n",
    "    idf = {\"test\":1/3, \"cat\":1/3, \"and\":0, \"dog\":1/3}\n",
    "    \n",
    "    words = sen.split()\n",
    "    words = [word for word in words if word in word2vec.vocab]\n",
    "    \n",
    "    idf_series = np.array([idf[word] for word in words])\n",
    "    print(\"idf shape \\t(1 x number of words)\\t\\t - \", idf_series.reshape(1, -1).shape)\n",
    "    idf_series = idf_series.reshape(1, -1)\n",
    "    \n",
    "    print(\"word2vec shape \\t(number of words x 300)\\t\\t - \", word2vec[words].shape)\n",
    "    result = np.matmul(idf_series.reshape(1, -1), word2vec[words])\n",
    "    result = result.reshape(-1)\n",
    "    print(\"result shape \\t(1-dimentional: np.array)\\t - \", result.shape)\n",
    "    \n",
    "    emb1 = word2vec[words].mean(axis=0)\n",
    "    emb2 = result\n",
    "    \n",
    "    assert emb1.all() == emb2.all()\n",
    "    # return result\n",
    "\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load(\"word2vec-google-news-300\")  \n",
    "\n",
    "test_word2vec_idf(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_centre(sentences, word2vec, idf=None, num_features=300):\n",
    "    \"\"\" Get sentences centre by averaging all embeddings of sentences in a list\n",
    "    \n",
    "    Depends on function get_sentence_vec()\n",
    "    \n",
    "    Args:\n",
    "        sentence (list): list of input sentences (str)\n",
    "        word2vec (dict): loaded word2vec model from Gensim\n",
    "        idf (dict, optional): inverse document frequency of words in all queries\n",
    "\n",
    "    Returns:\n",
    "        emb (np.array): 300-dimentions embedding of sentence\n",
    "    \"\"\"\n",
    "    # convert list of sentences to their vectors\n",
    "    sentences_vec = [get_sentence_vec(sentence, word2vec, idf) for sentence in sentences]\n",
    "    \n",
    "    # each row in matrix is 300 dimensions embedding of a sentence\n",
    "    sentences_matrix = np.vstack(sentences_vec)\n",
    "    # print(sentences_matrix.shape)\n",
    "    \n",
    "    # average of all rows, take mean at y-axis\n",
    "    sentences_centre = sentences_matrix.mean(axis=0)\n",
    "    \n",
    "    # result should be (300,) same as single sentence\n",
    "    # print(sentences_centre.shape)\n",
    "    return sentences_centre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load(\"word2vec-google-news-300\")  \n",
    "\n",
    "test_sentence_list = [\"hello world\", \"test cat and dog\"]\n",
    "get_sentences_centre(test_sentence_list, word2vec).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get intent list from dataframe[\"intent\"]\n",
    "# intent_list = df.intent.unique().tolist()\n",
    "def get_cluster_centre(df, intent_list, word2vec, idf=None):\n",
    "    \"\"\" get intent cluster centre based on intent list and word embeddings\n",
    "    \n",
    "    Depends on function get_sentences_centre()\n",
    "    \n",
    "    Args:\n",
    "        intent_list (list): List of unique intents(str)\n",
    "        word2vec (dict): word embeddings dictionary \n",
    "\n",
    "    Returns:\n",
    "        result (dict): intent cluster centres in dictionary format - {intent1:embedding1, intent2:embedding2,...}\n",
    "    \"\"\" \n",
    "    result = {intent:get_sentences_centre(df[df.intent == intent]['query'].values, word2vec, idf) for intent in intent_list}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "data = {'intent':  ['hello', 'cat'],\n",
    "        'query': ['hello world', 'test cat and dog']\n",
    "        }\n",
    "\n",
    "test_df = pd.DataFrame (data, columns = ['intent','query'])\n",
    "test_intents = ['hello', 'cat']\n",
    "\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load(\"word2vec-google-news-300\") \n",
    "\n",
    "result = get_cluster_centre(test_df, test_intents, word2vec)\n",
    "result[test_intents[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(df_in, word2vec, leave_one_out=False, idf_flag=False):\n",
    "    \"\"\" Get distance for each query to every intent center\n",
    "    \n",
    "    Depends on function get_cluster_centre()\n",
    "    \n",
    "    Args:\n",
    "        df_in (pd.DataFrame): input dataframe with intent and query\n",
    "        word2vec (dict): word embeddings dictionary \n",
    "        leave_one_out (bool): whether leave the input query out of training\n",
    "        idf (bool): whether use weighted word vectors to get sentence embedding\n",
    "\n",
    "    Returns:\n",
    "        result (pd.DataFrame): distance matrix for each query, lowest distance intent idealy should match label\n",
    "    \"\"\"\n",
    "    df = df_in.copy()\n",
    "    intent_list = df.intent.unique().tolist()\n",
    "    \n",
    "    if leave_one_out:\n",
    "        # print(\"Leave one out\")\n",
    "        sentence_distance = []\n",
    "        \n",
    "        for ind in df.index:\n",
    "            sentence_distance_tmp = []\n",
    "            query = df.loc[ind, 'query']\n",
    "            df_data = df.drop(ind)\n",
    "            \n",
    "            if idf_flag:\n",
    "                idf = get_idf_TfidfVectorizer(df['query'].tolist())\n",
    "            else:\n",
    "                # print(\"No IDF\")\n",
    "                idf = None\n",
    "            \n",
    "            sentence_centre_dic = get_cluster_centre(df_data, intent_list, word2vec, idf)\n",
    "            for intent in intent_list:\n",
    "                sentence_distance_tmp.append(cosine_distances(get_sentence_vec(query, word2vec, idf).reshape(1,-1), \n",
    "                                                              sentence_centre_dic[intent].reshape(1,-1)).item())\n",
    "            sentence_distance.append(sentence_distance_tmp)\n",
    "\n",
    "        df_sentence_distance = pd.DataFrame(sentence_distance, columns=intent_list)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        result = pd.concat([df, df_sentence_distance], axis=1)\n",
    "    \n",
    "    else:\n",
    "        sentence_centre_dic = get_cluster_centre(df, intent_list, word2vec)\n",
    "        # build dataframe that contains distance between each query to all intent cluster centre\n",
    "        for intent in intent_list:\n",
    "            # distance = cosine_similarity(sentence embedding, intent cluster centre embedding)\n",
    "            df[intent] = df['query'].apply(lambda x: cosine_distances(get_sentence_vec(x, word2vec).reshape(1,-1), \n",
    "                                                                      sentence_centre_dic[intent].reshape(1,-1)).item())\n",
    "        result = df\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_distance_matrix(df_in):\n",
    "    \"\"\" Evaluate distance matrix by compare closest intent center and label \"\"\"\n",
    "    df = df_in.copy()\n",
    "    df.set_index(['intent', 'query'], inplace=True)\n",
    "    df['cluster'] = df.idxmin(axis=1)\n",
    "    df.reset_index(inplace=True)\n",
    "    df['correct'] = (df.cluster == df.intent)\n",
    "    accuracy = sum(df.correct) / len(df)\n",
    "    # print(\"Accuracy for distance-based classification is\", '{:.2%}'.format(result))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "def test_clustering_accuracy(df_in, word2vec):\n",
    "    \"\"\" test accuracy based on distance of sentence to each cluster center\"\"\"\n",
    "    df_result = get_distance_matrix(df_in, word2vec)\n",
    "    # print(df_result.head())\n",
    "    accuracy = evaluate_distance_matrix(df_result)\n",
    "    return df_result, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for text embedding distance is 90.36%\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load(\"word2vec-google-news-300\")  \n",
    "\n",
    "df_result, accuracy = test_clustering_accuracy(df, word2vec)\n",
    "print(\"Accuracy for text embedding distance is\", '{:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "def test_leave_one_out_acc(df_in, word2vec):\n",
    "    df_result = get_distance_matrix(df_in, word2vec, leave_one_out=True)\n",
    "    # print(df_result.head())\n",
    "    accuracy = evaluate_distance_matrix(df_result)\n",
    "    return df_result, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for leave one out is 77.66%\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load(\"word2vec-google-news-300\")  \n",
    "\n",
    "df_result, accuracy = test_leave_one_out_acc(df, word2vec)\n",
    "print(\"Accuracy for leave one out is\", '{:.2%}'.format(accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "def test_idf_acc(df_in, word2vec):\n",
    "    df_result = get_distance_matrix(df_in, word2vec, leave_one_out=True, idf_flag=True)\n",
    "    # print(df_result.head())\n",
    "    accuracy = evaluate_distance_matrix(df_result)\n",
    "    return df_result, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further preprocessing same as XX\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stop_words = list(STOP_WORDS)\n",
    "\n",
    "df['query'] = df['query'].apply(lambda x:' '.join([token.lemma_ for token in nlp(x) if token.lemma_ not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for leave one out & use IDF is 86.04%\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load(\"word2vec-google-news-300\")  \n",
    "\n",
    "df_result, accuracy = test_idf_acc(df, word2vec)\n",
    "print(\"Accuracy for leave one out & use IDF is\", '{:.2%}'.format(accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>query</th>\n",
       "      <th>Promotions</th>\n",
       "      <th>Card Promotions</th>\n",
       "      <th>Open Account</th>\n",
       "      <th>OCBC Singapore Account</th>\n",
       "      <th>OCBC Securities Account</th>\n",
       "      <th>OCBC Malaysia Account</th>\n",
       "      <th>NISP Account</th>\n",
       "      <th>Card Cancellation</th>\n",
       "      <th>...</th>\n",
       "      <th>Credit card application rejection</th>\n",
       "      <th>Rebates</th>\n",
       "      <th>How to redeem rewards</th>\n",
       "      <th>360 Account interest dispute</th>\n",
       "      <th>Statement Request</th>\n",
       "      <th>Passbook savings account statement</th>\n",
       "      <th>Credit card statement</th>\n",
       "      <th>Debit card statement</th>\n",
       "      <th>Investment account statement</th>\n",
       "      <th>Update details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>promotion -PRON- ?</td>\n",
       "      <td>0.054397</td>\n",
       "      <td>0.150561</td>\n",
       "      <td>0.852556</td>\n",
       "      <td>0.888380</td>\n",
       "      <td>0.853222</td>\n",
       "      <td>0.854053</td>\n",
       "      <td>0.825240</td>\n",
       "      <td>0.778786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754952</td>\n",
       "      <td>0.715897</td>\n",
       "      <td>0.787883</td>\n",
       "      <td>0.714959</td>\n",
       "      <td>0.865170</td>\n",
       "      <td>0.829989</td>\n",
       "      <td>0.798031</td>\n",
       "      <td>0.861068</td>\n",
       "      <td>0.824732</td>\n",
       "      <td>0.913722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>promotion available ?</td>\n",
       "      <td>0.398314</td>\n",
       "      <td>0.367402</td>\n",
       "      <td>0.791127</td>\n",
       "      <td>0.828506</td>\n",
       "      <td>0.799017</td>\n",
       "      <td>0.785015</td>\n",
       "      <td>0.720876</td>\n",
       "      <td>0.832902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701948</td>\n",
       "      <td>0.721365</td>\n",
       "      <td>0.823370</td>\n",
       "      <td>0.708754</td>\n",
       "      <td>0.737345</td>\n",
       "      <td>0.740767</td>\n",
       "      <td>0.786862</td>\n",
       "      <td>0.790735</td>\n",
       "      <td>0.819249</td>\n",
       "      <td>0.756076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>promotion</td>\n",
       "      <td>0.054397</td>\n",
       "      <td>0.150561</td>\n",
       "      <td>0.852556</td>\n",
       "      <td>0.888380</td>\n",
       "      <td>0.853222</td>\n",
       "      <td>0.854053</td>\n",
       "      <td>0.825240</td>\n",
       "      <td>0.778786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754952</td>\n",
       "      <td>0.715897</td>\n",
       "      <td>0.787883</td>\n",
       "      <td>0.714959</td>\n",
       "      <td>0.865170</td>\n",
       "      <td>0.829989</td>\n",
       "      <td>0.798031</td>\n",
       "      <td>0.861068</td>\n",
       "      <td>0.824732</td>\n",
       "      <td>0.913722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>-PRON- want promotion</td>\n",
       "      <td>0.109674</td>\n",
       "      <td>0.180680</td>\n",
       "      <td>0.707255</td>\n",
       "      <td>0.759581</td>\n",
       "      <td>0.753057</td>\n",
       "      <td>0.782716</td>\n",
       "      <td>0.656011</td>\n",
       "      <td>0.691250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652758</td>\n",
       "      <td>0.650996</td>\n",
       "      <td>0.753304</td>\n",
       "      <td>0.636704</td>\n",
       "      <td>0.703462</td>\n",
       "      <td>0.664049</td>\n",
       "      <td>0.675467</td>\n",
       "      <td>0.746608</td>\n",
       "      <td>0.727109</td>\n",
       "      <td>0.804883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Promotions</td>\n",
       "      <td>view promotion</td>\n",
       "      <td>0.310240</td>\n",
       "      <td>0.307525</td>\n",
       "      <td>0.701131</td>\n",
       "      <td>0.741588</td>\n",
       "      <td>0.707859</td>\n",
       "      <td>0.729585</td>\n",
       "      <td>0.671988</td>\n",
       "      <td>0.767357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684919</td>\n",
       "      <td>0.657583</td>\n",
       "      <td>0.736120</td>\n",
       "      <td>0.617589</td>\n",
       "      <td>0.731495</td>\n",
       "      <td>0.739908</td>\n",
       "      <td>0.643090</td>\n",
       "      <td>0.754707</td>\n",
       "      <td>0.742063</td>\n",
       "      <td>0.787432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       intent                  query  Promotions  Card Promotions  \\\n",
       "0  Promotions     promotion -PRON- ?    0.054397         0.150561   \n",
       "1  Promotions  promotion available ?    0.398314         0.367402   \n",
       "2  Promotions              promotion    0.054397         0.150561   \n",
       "3  Promotions  -PRON- want promotion    0.109674         0.180680   \n",
       "4  Promotions         view promotion    0.310240         0.307525   \n",
       "\n",
       "   Open Account  OCBC Singapore Account  OCBC Securities Account   \\\n",
       "0      0.852556                0.888380                  0.853222   \n",
       "1      0.791127                0.828506                  0.799017   \n",
       "2      0.852556                0.888380                  0.853222   \n",
       "3      0.707255                0.759581                  0.753057   \n",
       "4      0.701131                0.741588                  0.707859   \n",
       "\n",
       "   OCBC Malaysia Account  NISP Account  Card Cancellation  ...  \\\n",
       "0               0.854053      0.825240           0.778786  ...   \n",
       "1               0.785015      0.720876           0.832902  ...   \n",
       "2               0.854053      0.825240           0.778786  ...   \n",
       "3               0.782716      0.656011           0.691250  ...   \n",
       "4               0.729585      0.671988           0.767357  ...   \n",
       "\n",
       "   Credit card application rejection   Rebates  How to redeem rewards  \\\n",
       "0                           0.754952  0.715897               0.787883   \n",
       "1                           0.701948  0.721365               0.823370   \n",
       "2                           0.754952  0.715897               0.787883   \n",
       "3                           0.652758  0.650996               0.753304   \n",
       "4                           0.684919  0.657583               0.736120   \n",
       "\n",
       "   360 Account interest dispute  Statement Request  \\\n",
       "0                      0.714959           0.865170   \n",
       "1                      0.708754           0.737345   \n",
       "2                      0.714959           0.865170   \n",
       "3                      0.636704           0.703462   \n",
       "4                      0.617589           0.731495   \n",
       "\n",
       "   Passbook savings account statement  Credit card statement  \\\n",
       "0                            0.829989               0.798031   \n",
       "1                            0.740767               0.786862   \n",
       "2                            0.829989               0.798031   \n",
       "3                            0.664049               0.675467   \n",
       "4                            0.739908               0.643090   \n",
       "\n",
       "   Debit card statement  Investment account statement  Update details  \n",
       "0              0.861068                      0.824732        0.913722  \n",
       "1              0.790735                      0.819249        0.756076  \n",
       "2              0.861068                      0.824732        0.913722  \n",
       "3              0.746608                      0.727109        0.804883  \n",
       "4              0.754707                      0.742063        0.787432  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result for next step\n",
    "from pathlib import Path\n",
    "CWD = Path.cwd()\n",
    "OUTPUT = CWD / \"output\"\n",
    "Path(OUTPUT).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv(\"data_leave_one_out.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alert]",
   "language": "python",
   "name": "conda-env-alert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
