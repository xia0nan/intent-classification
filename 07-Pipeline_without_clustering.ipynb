{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline without Text Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Import\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PATH_HOME = Path.home()\n",
    "PATH_PROJ = Path.cwd()\n",
    "PATH_DATA = PATH_PROJ\n",
    "\n",
    "sys.path.append(str(PATH_PROJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(641, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Statement request</td>\n",
       "      <td>i would like a copy of my statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Statement request</td>\n",
       "      <td>please send me a copy of my statement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label             Intent                              Questions\n",
       "0      0  Statement request    i would like a copy of my statement\n",
       "1      0  Statement request  please send me a copy of my statement"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN\n",
    "df_train = pd.read_csv('data2.csv')\n",
    "df_train.dropna(inplace=True)\n",
    "print(df_train.shape)\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Statement request</td>\n",
       "      <td>i would like a copy of my statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Statement request</td>\n",
       "      <td>please send me a copy of my statement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              intent                                  query\n",
       "0  Statement request    i would like a copy of my statement\n",
       "1  Statement request  please send me a copy of my statement"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename dataframe\n",
    "df_train = df_train.rename(columns={'Intent': 'intent', 'Questions': 'query'})\n",
    "df_train = df_train[['intent', 'query']]\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>User Clicked intent</th>\n",
       "      <th>Google-intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how do i submit a dispute?</td>\n",
       "      <td>Cancel credit card transaction</td>\n",
       "      <td>Dispute status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I lost my card</td>\n",
       "      <td>Lost or compromised cards</td>\n",
       "      <td>Lost or compromised cards</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Question             User Clicked intent  \\\n",
       "0  how do i submit a dispute?   Cancel credit card transaction   \n",
       "1               I lost my card       Lost or compromised cards   \n",
       "\n",
       "               Google-intent  \n",
       "0             Dispute status  \n",
       "1  Lost or compromised cards  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "df_test = pd.read_csv('uat_data_intent.csv')\n",
    "df_test.dropna(inplace=True)\n",
    "print(df_test.shape)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>User Clicked intent</th>\n",
       "      <th>Google-intent</th>\n",
       "      <th>correct_google</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how do i submit a dispute?</td>\n",
       "      <td>Cancel credit card transaction</td>\n",
       "      <td>Dispute status</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I lost my card</td>\n",
       "      <td>Lost or compromised cards</td>\n",
       "      <td>Lost or compromised cards</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have not received my purchases from the merc...</td>\n",
       "      <td>Cancel credit card transaction</td>\n",
       "      <td>Cancel ATM Card</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i have a transaction that i did not do</td>\n",
       "      <td>Cancel credit card transaction</td>\n",
       "      <td>Cancel credit card transaction</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how to terminate my card?</td>\n",
       "      <td>Cancel Credit or Debit Card</td>\n",
       "      <td>Card Cancellation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0                        how do i submit a dispute?    \n",
       "1                                     I lost my card   \n",
       "2  I have not received my purchases from the merc...   \n",
       "3             i have a transaction that i did not do   \n",
       "4                          how to terminate my card?   \n",
       "\n",
       "              User Clicked intent                   Google-intent  \\\n",
       "0  Cancel credit card transaction                  Dispute status   \n",
       "1       Lost or compromised cards       Lost or compromised cards   \n",
       "2  Cancel credit card transaction                 Cancel ATM Card   \n",
       "3  Cancel credit card transaction  Cancel credit card transaction   \n",
       "4     Cancel Credit or Debit Card               Card Cancellation   \n",
       "\n",
       "   correct_google  \n",
       "0               0  \n",
       "1               1  \n",
       "2               0  \n",
       "3               1  \n",
       "4               0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['correct_google'] = np.where(df_test['User Clicked intent'] == df_test['Google-intent'], 1, 0)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cancel credit card transaction</td>\n",
       "      <td>how do i submit a dispute?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lost or compromised cards</td>\n",
       "      <td>I lost my card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           intent                        query\n",
       "0  Cancel credit card transaction  how do i submit a dispute? \n",
       "1       Lost or compromised cards               I lost my card"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename dataframe\n",
    "df_test = df_test.rename(columns={'User Clicked intent': 'intent', 'Question': 'query'})\n",
    "df_test = df_test[['intent', 'query']]\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" Basic text cleaning\n",
    "        \n",
    "        1. lowercase\n",
    "        2. remove special characters\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenize(text):\n",
    "    \"\"\" tokenize text using NLTK and join back as sentence\"\"\"\n",
    "    # import nltk\n",
    "    # nltk.download('punkt')\n",
    "    return ' '.join(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for spacy tokenizer\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing questions\n",
    "df_train['query'] = df_train['query'].apply(clean_text)\n",
    "df_train['query'] = df_train['query'].apply(nltk_tokenize)\n",
    "df_train['query'] = df_train['query'].apply(lambda x:' '.join([token.lemma_ for token in nlp(x) if token.lemma_ not in stop_words]))\n",
    "df_train['query'] = df_train['query'].str.lower()\n",
    "\n",
    "\n",
    "# preprocessing test as well\n",
    "df_test['query'] = df_test['query'].apply(clean_text)\n",
    "df_test['query'] = df_test['query'].apply(nltk_tokenize)\n",
    "df_test['query'] = df_test['query'].apply(lambda x:' '.join([token.lemma_ for token in nlp(x) if token.lemma_ not in stop_words]))\n",
    "df_test['query'] = df_test['query'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Statement request</td>\n",
       "      <td>like copy -pron- statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Statement request</td>\n",
       "      <td>send -pron- copy -pron- statement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              intent                              query\n",
       "0  Statement request         like copy -pron- statement\n",
       "1  Statement request  send -pron- copy -pron- statement"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cancel credit card transaction</td>\n",
       "      <td>submit dispute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lost or compromised cards</td>\n",
       "      <td>lose -pron- card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           intent             query\n",
       "0  Cancel credit card transaction    submit dispute\n",
       "1       Lost or compromised cards  lose -pron- card"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Statement request', 'Passbook savings accounts']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_list = df_train.intent.unique().tolist()\n",
    "intent_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = intent_list.copy()\n",
    "intent2index = {v: i for (i, v) in enumerate(intents)}\n",
    "index2intent = {y:x for x,y in intent2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_intent_list = df_test.intent.unique().tolist()\n",
    "set(intent_list) == set(test_intent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "try:\n",
    "    word2vec\n",
    "except NameError:\n",
    "    word2vec = api.load(\"word2vec-google-news-300\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(intent_list, stop_words):\n",
    "    \"\"\" Get list of keywords from intent \"\"\"\n",
    "    keywords = []\n",
    "    for intent in list(set(intent_list)):\n",
    "        keywords.extend(intent.strip().split(' '))\n",
    "    keyword_list = list(set(keywords))\n",
    "    keyword_list = [i.lower() for i in keyword_list if i.lower() not in stop_words]\n",
    "    keyword_list.append('nsip')\n",
    "\n",
    "    keyword_list_lemma = []\n",
    "    text = nlp(' '.join([w for w in keyword_list]))\n",
    "    for token in text:\n",
    "        keyword_list_lemma.append(token.lemma_)\n",
    "    return keyword_list_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list_lemma = get_keywords(intent_list, stop_words=STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp_features(df, keyword_list_lemma):\n",
    "    \"\"\" Get keyword features from dataframe \"\"\"\n",
    "    data = df.copy()\n",
    "    data['lemma'] = data['query'].apply(lambda x:' '.join([token.lemma_ for token in nlp(x) if token.lemma_ not in stop_words]))\n",
    "    data['keyword'] = data['lemma'].apply(lambda x: list(set([token.lemma_ for token in nlp(x) if token.lemma_ in keyword_list_lemma])))\n",
    "\n",
    "    data['noun'] = data['query'].apply(lambda x: list(set([token.lemma_ for token in nlp(x) if token.pos_ in ['NOUN','PROPN'] and token.lemma_ not in stop_words])))\n",
    "    data['verb'] = data['query'].apply(lambda x: list(set([token.lemma_ for token in nlp(x) if token.pos_ in ['VERB'] and token.lemma_ not in stop_words])))\n",
    "\n",
    "    data['noun'] = data['noun'].apply(lambda x: ' '.join([w for w in x]))\n",
    "    data['verb'] = data['verb'].apply(lambda x: ' '.join([w for w in x]))\n",
    "    data['keyword'] = data['keyword'].apply(lambda x: ' '.join([w for w in x]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>query</th>\n",
       "      <th>lemma</th>\n",
       "      <th>keyword</th>\n",
       "      <th>noun</th>\n",
       "      <th>verb</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Statement request</td>\n",
       "      <td>like copy -pron- statement</td>\n",
       "      <td>like copy -pron- statement</td>\n",
       "      <td>statement</td>\n",
       "      <td>statement -pron- copy</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Statement request</td>\n",
       "      <td>send -pron- copy -pron- statement</td>\n",
       "      <td>send -pron- copy -pron- statement</td>\n",
       "      <td>statement</td>\n",
       "      <td>statement -pron-</td>\n",
       "      <td>send copy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              intent                              query  \\\n",
       "0  Statement request         like copy -pron- statement   \n",
       "1  Statement request  send -pron- copy -pron- statement   \n",
       "\n",
       "                               lemma    keyword                   noun  \\\n",
       "0         like copy -pron- statement  statement  statement -pron- copy   \n",
       "1  send -pron- copy -pron- statement  statement       statement -pron-   \n",
       "\n",
       "        verb  target  \n",
       "0                  0  \n",
       "1  send copy       0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = get_nlp_features(df_train, keyword_list_lemma)\n",
    "df_train['target'] = df_train['intent'].apply(lambda x: intent2index[x])\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>query</th>\n",
       "      <th>lemma</th>\n",
       "      <th>keyword</th>\n",
       "      <th>noun</th>\n",
       "      <th>verb</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cancel credit card transaction</td>\n",
       "      <td>submit dispute</td>\n",
       "      <td>submit dispute</td>\n",
       "      <td>dispute</td>\n",
       "      <td>dispute</td>\n",
       "      <td>submit</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lost or compromised cards</td>\n",
       "      <td>lose -pron- card</td>\n",
       "      <td>lose -pron- card</td>\n",
       "      <td>lose card</td>\n",
       "      <td>-pron- card</td>\n",
       "      <td>lose</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           intent             query             lemma  \\\n",
       "0  Cancel credit card transaction    submit dispute    submit dispute   \n",
       "1       Lost or compromised cards  lose -pron- card  lose -pron- card   \n",
       "\n",
       "     keyword         noun    verb  target  \n",
       "0    dispute      dispute  submit      17  \n",
       "1  lose card  -pron- card    lose      43  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = get_nlp_features(df_test, keyword_list_lemma)\n",
    "df_test['target'] = df_test['intent'].apply(lambda x: intent2index[x])\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvector_cols = ['lemma', 'keyword', 'noun', 'verb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(df_train, df_test, feature_cols):\n",
    "    \"\"\" split dataset, get X_train, X_test, y_train, y_test \"\"\"\n",
    "    X_train = df_train[feature_cols]\n",
    "    # print(X_train.head(1))\n",
    "    y_train = df_train['target']\n",
    "    # print(y_train.head(1))\n",
    "    X_test = df_test[feature_cols]\n",
    "    y_test = df_test['target']\n",
    "    # print(X_test.head(1))\n",
    "    # print(y_test.head(1))\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = get_train_test(df_train, df_test, feature_cols=countvector_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nlp_to_x(X_train, X_test):\n",
    "    \"\"\" Add NLP features to input X \"\"\"\n",
    "    v_lemma = TfidfVectorizer()\n",
    "    x_train_lemma = v_lemma.fit_transform(X_train['lemma'])\n",
    "    x_test_lemma = v_lemma.transform(X_test['lemma'])\n",
    "    vocab_lemma = dict(v_lemma.vocabulary_)\n",
    "\n",
    "    v_keyword = TfidfVectorizer()\n",
    "    x_train_keyword = v_keyword.fit_transform(X_train['keyword'])\n",
    "    x_test_keyword = v_keyword.transform(X_test['keyword'])\n",
    "    vocab_keyword = dict(v_keyword.vocabulary_)\n",
    "\n",
    "    v_noun = TfidfVectorizer()\n",
    "    x_train_noun = v_noun.fit_transform(X_train['noun'])\n",
    "    x_test_noun = v_noun.transform(X_test['noun'])\n",
    "    vocab_noun = dict(v_noun.vocabulary_)\n",
    "\n",
    "    v_verb = TfidfVectorizer()\n",
    "    x_train_verb = v_verb.fit_transform(X_train['verb'])\n",
    "    x_test_verb = v_verb.transform(X_test['verb'])\n",
    "    vocab_verb = dict(v_verb.vocabulary_)\n",
    "    \n",
    "    # combine all features \n",
    "    x_train_combined = hstack((x_train_lemma,\n",
    "                               x_train_keyword,\n",
    "                               x_train_noun,\n",
    "                               x_train_verb),format='csr')\n",
    "    x_train_combined_columns= v_lemma.get_feature_names()+\\\n",
    "                            v_keyword.get_feature_names()+\\\n",
    "                            v_noun.get_feature_names()+\\\n",
    "                            v_verb.get_feature_names()\n",
    "\n",
    "    x_test_combined  = hstack((x_test_lemma, \n",
    "                               x_test_keyword, \n",
    "                               x_test_noun, \n",
    "                               x_test_verb), format='csr')\n",
    "    x_test_combined_columns = v_lemma.get_feature_names()+\\\n",
    "                            v_keyword.get_feature_names()+\\\n",
    "                            v_noun.get_feature_names()+\\\n",
    "                            v_verb.get_feature_names()\n",
    "\n",
    "    x_train_combined = pd.DataFrame(x_train_combined.toarray())\n",
    "    x_train_combined.columns = x_train_combined_columns\n",
    "\n",
    "    x_test_combined = pd.DataFrame(x_test_combined.toarray())\n",
    "    x_test_combined.columns = x_test_combined_columns\n",
    "    \n",
    "    return x_train_combined, x_test_combined, v_lemma, v_keyword, v_noun, v_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_combined, x_test_combined, v_lemma, v_keyword, v_noun, v_verb = add_nlp_to_x(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=50, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build classifier\n",
    "clf = RandomForestClassifier(max_depth=50, n_estimators=1000)\n",
    "clf.fit(x_train_combined, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(x_test_combined)\n",
    "best_3 = pd.DataFrame(np.argsort(probs, axis=1)[:,-3:],columns=['top3','top2','top1'])\n",
    "best_3['top1'] = clf.classes_[best_3['top1']]\n",
    "best_3['top2'] = clf.classes_[best_3['top2']]\n",
    "best_3['top3'] = clf.classes_[best_3['top3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([best_3.reset_index(drop=True),\n",
    "                    pd.DataFrame(y_test).reset_index(drop=True), \n",
    "                    X_test[countvector_cols].reset_index(drop=True)], axis=1)\n",
    "score_1 = result[result['top1'] == result['target']].shape[0] / result.shape[0]\n",
    "score_2 = result[(result['top1'] == result['target']) | (result['top2'] == result['target'])].shape[0] / result.shape[0]\n",
    "score_3 = result[(result['top1'] == result['target']) | (result['top2'] == result['target'])| (result['top3'] == result['target'])].shape[0] / result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for top 1 clustering + classifier result is 70.3%\n",
      "Accuracy for top 2 clustering + classifier result is 79.7%\n",
      "Accuracy for top 3 clustering + classifier result is 85.2%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for top 1 clustering + classifier result is {:.1%}'.format(score_1))\n",
    "print('Accuracy for top 2 clustering + classifier result is {:.1%}'.format(score_2))\n",
    "print('Accuracy for top 3 clustering + classifier result is {:.1%}'.format(score_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "model_filename = 'RFClassifier2.pkl'\n",
    "pickle.dump(clf, open(model_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vectorizer\n",
    "with open('TFIDFVectorizer_lemma2.pkl', 'wb') as f:\n",
    "    pickle.dump(v_lemma, f)\n",
    "with open('TFIDFVectorizer_keyword2.pkl', 'wb') as f:\n",
    "    pickle.dump(v_keyword, f)\n",
    "with open('TFIDFVectorizer_noun2.pkl', 'wb') as f:\n",
    "    pickle.dump(v_noun, f)\n",
    "with open('TFIDFVectorizer_verb2.pkl', 'wb') as f:\n",
    "    pickle.dump(v_verb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save necessary variables\n",
    "with open('intent_list2.pkl', 'wb') as f:\n",
    "    pickle.dump(intent_list, f)\n",
    "with open('intent2index2.pkl', 'wb') as f:\n",
    "    pickle.dump(intent2index, f)\n",
    "with open('keyword_list_lemma2.pkl', 'wb') as f:\n",
    "    pickle.dump(keyword_list_lemma, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alert]",
   "language": "python",
   "name": "conda-env-alert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
